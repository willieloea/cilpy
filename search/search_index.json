{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to <code>cilpy</code>","text":"<p><code>cilpy</code> (which stands for \"computational intelligence library for Python\") is an extensible Python library for computational intelligence, designed to streamline research and experimentation with nature-inspired algorithms (NIAs).</p> <p>The primary goal of <code>cilpy</code> is to provide a unified and robust framework for tackling a wide range of optimization tasks, including:</p> <ul> <li>Single- and multi-objective optimization</li> <li>Constrained and unconstrained problems</li> <li>Static and dynamic objectives and constraints</li> </ul> <p><code>cilpy</code> strives to be a useful tool for researchers developing new algorithms, and students learning about computational intelligence. <code>cilpy</code> offers tools to design, execute, and analyze experiments with ease and rigor.</p>"},{"location":"#the-cilpy-philosophy-modularity-and-extensibility","title":"The <code>cilpy</code> Philosophy: Modularity and Extensibility","text":"<p>The ecosystem for nature-inspired optimization algorithms is fragmented. Researchers frequently have to build their experimental setups from scratch or adapt to libraries that are difficult to extend, lack comprehensive features, or are no longer maintained.</p> <p><code>cilpy</code> is designed to solve this problem by separating concerns. The library is divided into four distinct, interoperable components:</p> <ol> <li><code>cilpy.problem</code>: This component is used to specify optimization problems,    including objective function(s), search space, and constraints.</li> <li><code>cilpy.solver</code>: This component is used to specify solvers (NIAs), that    search for solutions to a given problem.</li> <li><code>cilpy.compare</code>: This component will provide the tools for statistical    analysis and visualization to compare the performance of different solvers on    various problems.</li> <li><code>cilpy.runner</code>: This component is used to orchestrate the interaction    between components, run experiments, and log results.</li> </ol> <p>By enforcing a clean separation between these parts through well-defined interfaces, <code>cilpy</code> allows users to seamlessly swap out components.</p> <ul> <li>Want to test a new algorithm? Implement the <code>Solver</code> interface, and you     can immediately benchmark it against all existing problems.</li> <li>Have a new benchmark problem? Implement the <code>Problem</code> interface, and any   existing solver can be used to tackle it.</li> </ul> <p>This modularity drastically reduces boilerplate code and allows users to focus on the novel aspects of their research.</p>"},{"location":"#how-cilpy-works-the-experiment-runner","title":"How <code>cilpy</code> Works: The Experiment Runner","text":"<p>At the core of <code>cilpy</code>'s workflow is the <code>ExperimentRunner</code> in <code>cilpy.runner</code>. It orchestrates the entire experimental process. After being provided a list of problems to solve, solvers and their configurations to test, and comparisons to perform on the algorithm, the runner orchestrates all further action, and performs the experiment.</p> <p>The typical workflow looks like this:</p> <ol> <li>Define Problems: Instantiate or create custom classes for the    optimization problems you want to investigate.</li> <li>Configure Solvers: Create a list of dictionaries, where each dictionary    specifies a solver's class (e.g., <code>GA</code>, <code>PSO</code>) and its parameters (e.g.,    <code>population_size</code>, <code>mutation_rate</code>).</li> <li>Specify Comparisons: TODO</li> <li>Configure the Runner: Initialize the <code>ExperimentRunner</code> with your    problems, solver configurations, comparisons to perform, and experiment    parameters (like the number of runs and iterations).</li> <li>Execute: Call the <code>run_experiments()</code> method. The runner will    systematically pair each solver with each problem, execute the specified    number of independent runs, and save the results for later analysis.</li> </ol> <p>This declarative approach makes experiments easy to define, reproduce, and modify.</p>"},{"location":"#how-to-use-this-documentation","title":"How to Use This Documentation","text":"<p>This documentation is structured to guide you, whether you are using the library for the first time or developing new components for it.</p> <ul> <li>Quickstart: A quick guide on how to implement the core   interfaces and run an experiment.</li> <li>Included: Discusses the problems, solvers, and analysis   tools that have been incorporated into <code>cilpy</code>.</li> <li>API Reference: A detailed, technical reference for all the   core classes and functions in the library.</li> <li>Developer Guide: For those who want to contribute to   <code>cilpy</code>, this contains information on our coding standards, testing   procedures, and how to get involved.</li> </ul>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>This guide shows you how to implement <code>cilpy</code>'s core interfaces and run a basic experiment.</p>"},{"location":"quickstart/#the-core-interfaces","title":"The Core Interfaces","text":"<p>To use <code>cilpy</code>, you need to understand two key interfaces:</p> <ol> <li><code>cilpy.problem.Problem</code>: Defines an optimization problem. It requires the    following methods:<ul> <li><code>__init__(self, dimension, bounds, name)</code>: Initializes a Problem instance.</li> <li><code>evaluate(self, solution)</code>: Evaluates a candidate solution.</li> <li><code>is_dynamic(self)</code>: Indicates if the problem changes over time.</li> </ul> </li> <li><code>cilpy.solver.Solver</code>: Defines an optimization algorithm. It requires the    following methods:<ul> <li><code>__init__(self, problem, name, constraint_handler, **kwargs)</code>:   Initializes the solver.</li> <li><code>step(self)</code>: Performs one iteration of the optimization algorithm.</li> <li><code>get_result(self)</code>: Returns the best solution(s) found so far.</li> </ul> </li> </ol>"},{"location":"quickstart/#writing-your-own-experiment","title":"Writing your own experiment","text":""},{"location":"quickstart/#step-1-implement-a-problem","title":"Step 1: Implement a <code>Problem</code>","text":"<p>Create a file named <code>my_problem.py</code>. Here, we'll implement the 2D Sphere function.</p> <pre><code># my_problem.py\nfrom typing import List, Tuple\nfrom cilpy.problem import Problem, Evaluation\n\nclass MySphere(Problem[List[float], float]):\n    \"\"\"A custom implementation of the Sphere function.\"\"\"\n    def __init__(self, dimension: int):\n        super().__init__(\n            dimension=dimension,\n            bounds=([-5.12] * dimension, [5.12] * dimension),\n            name=\"MySphere\"\n        )\n\n    def evaluate(self, solution: List[float]) -&gt; Evaluation[float]:\n        \"\"\"Calculates the sum of squares of the solution's elements.\"\"\"\n        fitness = sum(x**2 for x in solution)\n        return Evaluation(fitness=fitness)\n\n    def is_dynamic(self) -&gt; Tuple[bool, bool]:\n        \"\"\"This problem is not dynamic.\"\"\"\n        return (False, False)\n</code></pre>"},{"location":"quickstart/#step-2-implement-a-solver","title":"Step 2: Implement a <code>Solver</code>","text":"<p>Create a file named <code>my_solver.py</code>. Here, we'll implement a simple Random Search algorithm.</p> <pre><code># my_solver.py\nimport random\nfrom typing import List, Tuple\nfrom cilpy.problem import Problem, Evaluation\nfrom cilpy.solver import Solver\n\nclass RandomSearch(Solver[List[float], float]):\n    \"\"\"A simple solver that generates random solutions.\"\"\"\n    def __init__(self, problem: Problem[List[float], float], name: str):\n        super().__init__(problem, name)\n        self.best_solution = None\n        self.best_evaluation = Evaluation(fitness=float('inf'))\n\n    def step(self) -&gt; None:\n        \"\"\"Generate one new random solution and update the best.\"\"\"\n        # Create a new random solution within the problem's bounds\n        lower, upper = self.problem.bounds\n        new_solution = [random.uniform(lower[i], upper[i])\n                        for i in range(self.problem.dimension)]\n\n        # Evaluate the new solution\n        new_evaluation = self.problem.evaluate(new_solution)\n\n        # If it's better than the current best, update\n        if new_evaluation.fitness &lt; self.best_evaluation.fitness:\n            self.best_solution = new_solution\n            self.best_evaluation = new_evaluation\n\n    def get_result(self) -&gt; List[Tuple[List[float], Evaluation[float]]]:\n        \"\"\"Return the best solution found so far.\"\"\"\n        return [(self.best_solution, self.best_evaluation)]\n</code></pre>"},{"location":"quickstart/#step-3-run-the-experiment","title":"Step 3: Run the Experiment","text":"<p>Now, use the <code>ExperimentRunner</code> to run your new solver on your new problem. Create <code>run_my_experiment.py</code>.</p> <pre><code># run_my_experiment.py\nfrom cilpy.runner import ExperimentRunner\nfrom my_problem import MySphere\nfrom my_solver import RandomSearch\n\n# 1. Define the Problem\nproblems_to_run = [\n    MySphere(dimension=2)\n]\n\n# 2. Configure the Solver\nsolver_configs = [\n    {\n        \"class\": RandomSearch,\n        \"params\": {\"name\": \"MyRandomSearch\"}\n    }\n]\n\n# 3. Set Experiment Parameters\nnumber_of_runs = 5\nmax_iter = 100\n\n# 4. Create and run the experiment\nrunner = ExperimentRunner(\n    problems=problems_to_run,\n    solver_configurations=solver_configs,\n    num_runs=number_of_runs,\n    max_iterations=max_iter\n)\nrunner.run_experiments()\n</code></pre> <p>Run this script from your terminal:</p> <pre><code>python run_my_experiment.py\n</code></pre> <p>This will create a <code>MySphere_MyRandomSearch.out.csv</code> file with the experiment's results.</p>"},{"location":"quickstart/#using-included-components","title":"Using Included Components","text":"<p>You don't always need to create new components. <code>cilpy</code> includes standard problems and solvers. Here is how you would run the included <code>GA</code> solver on the included <code>Ackley</code> problem.</p> <pre><code># run_included_experiment.py\nfrom cilpy.runner import ExperimentRunner\nfrom cilpy.problem.functions import Ackley  # Included problem\nfrom cilpy.solver.ga import GA              # Included solver\n\nrunner = ExperimentRunner(\n    problems=[Ackley(dimension=10)],\n    solver_configurations=[\n        {\n            \"class\": GA,\n            \"params\": {\n                \"name\": \"GA_Standard\",\n                \"population_size\": 50,\n                \"crossover_rate\": 0.8,\n                \"mutation_rate\": 0.1,\n            }\n        }\n    ],\n    num_runs=10,\n    max_iterations=1000\n)\nrunner.run_experiments()\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>The <code>cilpy</code> API is designed for extensibility. By implementing these core abstract classes, your custom components will integrate seamlessly with the library's ecosystem.</p>"},{"location":"api/#problem-api","title":"Problem API","text":"<p>The <code>Problem</code> interface is the standard for defining optimization problems. Any class that inherits from <code>cilpy.problem.Problem</code> can be passed to any <code>cilpy</code> solver.</p> <p>Key Responsibilities:</p> <ul> <li>Define the search space (dimensionality and bounds).  </li> <li>Implement the <code>evaluate()</code> method, which takes a candidate solution and   returns its fitness and constraint violations.  </li> <li>Indicate whether the objectives and constraints of the problem are dynamic via   the <code>is_dynamic()</code> method.</li> </ul> <p>\u00bb View Full Problem API</p>"},{"location":"api/#solver-api","title":"Solver API","text":"<p>The <code>Solver</code> interface is the standard for implementing optimization algorithms. Any class that inherits from <code>cilpy.solver.Solver</code> can be used by the <code>ExperimentRunner</code> to solve any <code>cilpy</code> problem.</p> <p>Key Responsibilities:</p> <ul> <li>Accept a <code>Problem</code> object during initialization.</li> <li>Implement the <code>step()</code> method, which executes a single iteration or generation   of the algorithm.</li> <li>Implement the <code>get_result()</code> method, which returns the best solution(s) found   by the solver.</li> </ul> <p>\u00bb View Full Solver API</p>"},{"location":"api/#compare-api","title":"Compare API","text":"<p>(This component is under development.)</p> <p>The <code>Compare</code> interface will provide a standard for creating tools that analyze and visualize experiment results.</p> <p>Key Responsibilities:</p> <ul> <li>Process output data generated by the <code>ExperimentRunner</code>.</li> <li>Perform statistical comparisons between solver performances.</li> <li>Generate plots and tables to summarize results.</li> </ul> <p>\u00bb View Full Compare API</p>"},{"location":"api/compare/","title":"Compare","text":"<p>The goal of this component is that it should allow users to conduct statistical tests to compare algorithms, much like this tool.</p>"},{"location":"api/constraint_handler/","title":"<code>cilpy.solver.chm.ConstraintHandler</code> Documentation","text":"<p>The constraint handling mechanism module: Defines the constraint handling mechanism interface.</p> <p>This module provides the abstract \"contract\" for all constraint handling mechanisms within the <code>cilpy</code> library.</p> <p>In the future, this module could be modified to be a comparator for multi-objective optimization problems.</p>"},{"location":"api/constraint_handler/#cilpy.solver.chm.ConstraintHandler","title":"<code>ConstraintHandler</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[FitnessType]</code></p> <p>An abstract interface for a constraint handling mechanism.</p> <p>This class defines the strategy for comparing two evaluations in the context of a constrained optimization problem. Solvers will delegate comparison logic to an implementation of this class.</p> Source code in <code>cilpy/solver/chm/__init__.py</code> <pre><code>class ConstraintHandler(ABC, Generic[FitnessType]):\n    \"\"\"\n    An abstract interface for a constraint handling mechanism.\n\n    This class defines the strategy for comparing two evaluations in the\n    context of a constrained optimization problem. Solvers will delegate\n    comparison logic to an implementation of this class.\n    \"\"\"\n\n    @abstractmethod\n    def is_better(\n        self, eval_a: Evaluation[FitnessType], eval_b: Evaluation[FitnessType]\n    ) -&gt; bool:\n        \"\"\"\n        Compares two evaluations to determine if `eval_a` is better than `eval_b`.\n\n        Args:\n            eval_a: The first evaluation.\n            eval_b: The second evaluation.\n\n        Returns:\n            True if `eval_a` is considered superior to `eval_b` according to the\n            specific constraint handling strategy, False otherwise.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/constraint_handler/#cilpy.solver.chm.ConstraintHandler.is_better","title":"<code>is_better(eval_a, eval_b)</code>  <code>abstractmethod</code>","text":"<p>Compares two evaluations to determine if <code>eval_a</code> is better than <code>eval_b</code>.</p> <p>Parameters:</p> Name Type Description Default <code>eval_a</code> <code>Evaluation[FitnessType]</code> <p>The first evaluation.</p> required <code>eval_b</code> <code>Evaluation[FitnessType]</code> <p>The second evaluation.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if <code>eval_a</code> is considered superior to <code>eval_b</code> according to the</p> <code>bool</code> <p>specific constraint handling strategy, False otherwise.</p> Source code in <code>cilpy/solver/chm/__init__.py</code> <pre><code>@abstractmethod\ndef is_better(\n    self, eval_a: Evaluation[FitnessType], eval_b: Evaluation[FitnessType]\n) -&gt; bool:\n    \"\"\"\n    Compares two evaluations to determine if `eval_a` is better than `eval_b`.\n\n    Args:\n        eval_a: The first evaluation.\n        eval_b: The second evaluation.\n\n    Returns:\n        True if `eval_a` is considered superior to `eval_b` according to the\n        specific constraint handling strategy, False otherwise.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/constraint_handler/#cilpy.solver.chm.DefaultComparator","title":"<code>DefaultComparator</code>","text":"<p>               Bases: <code>ConstraintHandler[float]</code></p> <p>A default comparator for single-objective, unconstrained problems.</p> <p>This handler performs a simple fitness comparison, assuming that a lower fitness value is better. It does not consider constraints.</p> Source code in <code>cilpy/solver/chm/__init__.py</code> <pre><code>class DefaultComparator(ConstraintHandler[float]):\n    \"\"\"\n    A default comparator for single-objective, unconstrained problems.\n\n    This handler performs a simple fitness comparison, assuming that a lower\n    fitness value is better. It does not consider constraints.\n    \"\"\"\n\n    def is_better(self, eval_a: Evaluation[float], eval_b: Evaluation[float]) -&gt; bool:\n        return eval_a.fitness &lt; eval_b.fitness\n</code></pre>"},{"location":"api/problem/","title":"<code>cilpy.problem</code> Documentation","text":"<p>The problem module: Defines the optimization problem interface.</p> <p>This module provides the abstract \"contract\" for all optimization problems within the <code>cilpy</code> library. It consists of two main components:</p> <ol> <li><code>Evaluation</code>: A dataclass that standardizes the return value from any problem    evaluation, capturing fitness and constraint information.</li> <li><code>Problem</code>: An abstract base class that defines the required methods and    attributes for a problem to be compatible with <code>cilpy</code> solvers.</li> </ol> <p>By implementing the <code>Problem</code> interface, users can define custom optimization landscapes that any solver in the library can operate on.</p>"},{"location":"api/problem/#cilpy.problem.Evaluation","title":"<code>Evaluation</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[FitnessType]</code></p> <p>A container for the results of a single problem evaluation.</p> <p>This dataclass standardizes the output of a problem's <code>evaluate</code> method, providing a consistent structure for fitness values and constraint violations that all <code>cilpy</code> solvers can understand.</p> <p>Attributes:</p> Name Type Description <code>fitness</code> <code>FitnessType</code> <p>The objective function value(s) of the solution. This can be a single float for single-objective problems or a list of floats for multi-objective problems.</p> <code>constraints_inequality</code> <code>Optional[List[float]]</code> <p>A list of values for inequality constraints. For a constraint <code>g(x) &lt;= 0</code>, a positive value indicates a violation. <code>None</code> if the problem has no inequality constraints.</p> <code>constraints_equality</code> <code>Optional[List[float]]</code> <p>A list of values for equality constraints. For a constraint <code>h(x) == 0</code>, any non-zero value indicates a violation. <code>None</code> if the problem has no equality constraints.</p> Example <p>.. code-block:: python</p> <pre><code># For a single-objective, unconstrained problem\neval_unconstrained = Evaluation(fitness=10.5)\n\n# For a multi-objective, constrained problem\neval_constrained = Evaluation(\n    fitness=[10.5, -2.1],\n    constraints_inequality=[0.5, -0.1], # First constraint violated\n    constraints_equality=[-0.01, 0.0]\n)\n</code></pre> Source code in <code>cilpy/problem/__init__.py</code> <pre><code>@dataclass\nclass Evaluation(Generic[FitnessType]):\n    \"\"\"A container for the results of a single problem evaluation.\n\n    This dataclass standardizes the output of a problem's `evaluate` method,\n    providing a consistent structure for fitness values and constraint\n    violations that all `cilpy` solvers can understand.\n\n    Attributes:\n        fitness: The objective function value(s) of the solution.\n            This can be a single float for single-objective problems or a list\n            of floats for multi-objective problems.\n        constraints_inequality: A list of values for inequality\n            constraints. For a constraint `g(x) &lt;= 0`, a positive value\n            indicates a violation. `None` if the problem has no inequality\n            constraints.\n        constraints_equality: A list of values for equality\n            constraints. For a constraint `h(x) == 0`, any non-zero value\n            indicates a violation. `None` if the problem has no equality\n            constraints.\n\n    Example:\n        .. code-block:: python\n\n            # For a single-objective, unconstrained problem\n            eval_unconstrained = Evaluation(fitness=10.5)\n\n            # For a multi-objective, constrained problem\n            eval_constrained = Evaluation(\n                fitness=[10.5, -2.1],\n                constraints_inequality=[0.5, -0.1], # First constraint violated\n                constraints_equality=[-0.01, 0.0]\n            )\n    \"\"\"\n\n    fitness: FitnessType\n    constraints_inequality: Optional[List[float]] = None\n    constraints_equality: Optional[List[float]] = None\n</code></pre>"},{"location":"api/problem/#cilpy.problem.Problem","title":"<code>Problem</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[SolutionType, FitnessType]</code></p> <p>An abstract interface for an optimization problem.</p> <p>This class serves as the blueprint for all problems in <code>cilpy</code>. To create a new problem, you must inherit from this class and implement its abstract methods. The interface is generic, allowing for various solution types (e.g., <code>List[float]</code>, <code>np.ndarray</code>) and fitness structures.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the problem instance.</p> <code>dimension</code> <code>int</code> <p>The number of decision variables in the solution space.</p> <code>bounds</code> <code>Tuple[SolutionType, SolutionType]</code> <p>A tuple `(lower_bounds,</p> Example <p>A minimal implementation for a 2D Sphere function problem.</p> <p>.. code-block:: python</p> <pre><code>from cilpy.problem import Problem, Evaluation\n\nclass SphereProblem(Problem[list[float], float]):\n    def __init__(self, dimension: int):\n        super().__init__(\n            dimension=dimension,\n            bounds=([-5.12] * dimension, [5.12] * dimension),\n            name=\"Sphere\"\n        )\n\n    def evaluate(self, solution: list[float]) -&gt; Evaluation[float]:\n        fitness = sum(x**2 for x in solution)\n        return Evaluation(fitness=fitness)\n\n    def is_dynamic(self) -&gt; tuple[bool, bool]:\n        return (False, False)\n</code></pre> Source code in <code>cilpy/problem/__init__.py</code> <pre><code>class Problem(ABC, Generic[SolutionType, FitnessType]):\n    \"\"\"An abstract interface for an optimization problem.\n\n    This class serves as the blueprint for all problems in `cilpy`. To create a\n    new problem, you must inherit from this class and implement its abstract\n    methods. The interface is generic, allowing for various solution types\n    (e.g., `List[float]`, `np.ndarray`) and fitness structures.\n\n    Attributes:\n        name (str): The name of the problem instance.\n        dimension (int): The number of decision variables in the solution space.\n        bounds (Tuple[SolutionType, SolutionType]): A tuple `(lower_bounds,\n        upper_bounds)` defining the search space for each dimension.\n\n    Example:\n        A minimal implementation for a 2D Sphere function problem.\n\n        .. code-block:: python\n\n            from cilpy.problem import Problem, Evaluation\n\n            class SphereProblem(Problem[list[float], float]):\n                def __init__(self, dimension: int):\n                    super().__init__(\n                        dimension=dimension,\n                        bounds=([-5.12] * dimension, [5.12] * dimension),\n                        name=\"Sphere\"\n                    )\n\n                def evaluate(self, solution: list[float]) -&gt; Evaluation[float]:\n                    fitness = sum(x**2 for x in solution)\n                    return Evaluation(fitness=fitness)\n\n                def is_dynamic(self) -&gt; tuple[bool, bool]:\n                    return (False, False)\n    \"\"\"\n\n    @abstractmethod\n    def __init__(\n        self, dimension: int, bounds: Tuple[SolutionType, SolutionType], name: str\n    ) -&gt; None:\n        \"\"\"Initializes a Problem instance.\n\n        Subclasses must call `super().__init__(...)` to ensure these core\n        attributes are set.\n\n        Args:\n            name: The name of the optimization problem.\n            dimension: The dimensionality of the solution space.\n            bounds: A tuple `(lower_bounds, upper_bounds)` defining the\n                feasible range for each decision variable. For a real-valued\n                problem, this is typically `([L1, L2, ...], [U1, U2, ...])`.\n        \"\"\"\n        self.name = name\n        self.dimension = dimension\n        self.bounds = bounds\n\n    @abstractmethod\n    def evaluate(self, solution: SolutionType) -&gt; Evaluation[FitnessType]:\n        \"\"\"Evaluates a candidate solution.\n\n        This is the core method of a problem. It takes a solution from a solver\n        and returns its performance and feasibility.\n\n        Args:\n            solution: The candidate solution to be evaluated. Its type\n                (e.g., `List[float]`, `np.ndarray`) must be consistent\n                with the `SolutionType` used in the class definition.\n\n        Returns:\n            An `Evaluation` object containing the fitness and constraint\n            violation information for the given solution.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_dynamic(self) -&gt; Tuple[bool, bool]:\n        \"\"\"Indicates if the problem's landscape changes over time.\n\n        Dynamic problems require specialized solvers that can adapt to\n        changes in the objective function or constraints.\n\n        Note:\n            Solvers can query this method to decide whether to employ\n            change-detection mechanisms or re-evaluate their archives of\n            best-so-far solutions.\n\n        Returns:\n            A tuple `(is_objective_dynamic, is_constraint_dynamic)` where:\n                - `is_objective_dynamic` is `True` if the objective\n                  function(s) change over time.\n                - `is_constraint_dynamic` is `True` if the constraint\n                  function(s) change over time.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def is_multi_objective(self) -&gt; bool:\n        \"\"\"Indicates if the problem has multiple objectives.\"\"\"\n        pass\n\n    def begin_iteration(self) -&gt; None:\n        \"\"\"\n        A notification called by the ExperimentRunner before each solver\n        iteration.\n\n        Dynamic problems should override this method to update their internal\n        state, such as an iteration counter, and to trigger environmental\n        changes. The default implementation does nothing.\n        \"\"\"\n        pass\n\n    def get_fitness_bounds(self) -&gt; Tuple[FitnessType, FitnessType]:\n        \"\"\"\n        Returns the known theoretical min and max fitness values for the\n        problem.\n\n        This is used for calculating normalized performance metrics.\n\n        Returns:\n            A tuple containing (global_minimum_fitness, global_maximum_fitness).\n        \"\"\"\n        raise NotImplementedError(\n            f\"The problem '{self.name}' does not have a known optimum value. \"\n            \"Implement get_fitness_bounds() to use metrics that require it.\"\n        )\n</code></pre>"},{"location":"api/problem/#cilpy.problem.Problem.__init__","title":"<code>__init__(dimension, bounds, name)</code>  <code>abstractmethod</code>","text":"<p>Initializes a Problem instance.</p> <p>Subclasses must call <code>super().__init__(...)</code> to ensure these core attributes are set.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the optimization problem.</p> required <code>dimension</code> <code>int</code> <p>The dimensionality of the solution space.</p> required <code>bounds</code> <code>Tuple[SolutionType, SolutionType]</code> <p>A tuple <code>(lower_bounds, upper_bounds)</code> defining the feasible range for each decision variable. For a real-valued problem, this is typically <code>([L1, L2, ...], [U1, U2, ...])</code>.</p> required Source code in <code>cilpy/problem/__init__.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self, dimension: int, bounds: Tuple[SolutionType, SolutionType], name: str\n) -&gt; None:\n    \"\"\"Initializes a Problem instance.\n\n    Subclasses must call `super().__init__(...)` to ensure these core\n    attributes are set.\n\n    Args:\n        name: The name of the optimization problem.\n        dimension: The dimensionality of the solution space.\n        bounds: A tuple `(lower_bounds, upper_bounds)` defining the\n            feasible range for each decision variable. For a real-valued\n            problem, this is typically `([L1, L2, ...], [U1, U2, ...])`.\n    \"\"\"\n    self.name = name\n    self.dimension = dimension\n    self.bounds = bounds\n</code></pre>"},{"location":"api/problem/#cilpy.problem.Problem.begin_iteration","title":"<code>begin_iteration()</code>","text":"<p>A notification called by the ExperimentRunner before each solver iteration.</p> <p>Dynamic problems should override this method to update their internal state, such as an iteration counter, and to trigger environmental changes. The default implementation does nothing.</p> Source code in <code>cilpy/problem/__init__.py</code> <pre><code>def begin_iteration(self) -&gt; None:\n    \"\"\"\n    A notification called by the ExperimentRunner before each solver\n    iteration.\n\n    Dynamic problems should override this method to update their internal\n    state, such as an iteration counter, and to trigger environmental\n    changes. The default implementation does nothing.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/problem/#cilpy.problem.Problem.evaluate","title":"<code>evaluate(solution)</code>  <code>abstractmethod</code>","text":"<p>Evaluates a candidate solution.</p> <p>This is the core method of a problem. It takes a solution from a solver and returns its performance and feasibility.</p> <p>Parameters:</p> Name Type Description Default <code>solution</code> <code>SolutionType</code> <p>The candidate solution to be evaluated. Its type (e.g., <code>List[float]</code>, <code>np.ndarray</code>) must be consistent with the <code>SolutionType</code> used in the class definition.</p> required <p>Returns:</p> Type Description <code>Evaluation[FitnessType]</code> <p>An <code>Evaluation</code> object containing the fitness and constraint</p> <code>Evaluation[FitnessType]</code> <p>violation information for the given solution.</p> Source code in <code>cilpy/problem/__init__.py</code> <pre><code>@abstractmethod\ndef evaluate(self, solution: SolutionType) -&gt; Evaluation[FitnessType]:\n    \"\"\"Evaluates a candidate solution.\n\n    This is the core method of a problem. It takes a solution from a solver\n    and returns its performance and feasibility.\n\n    Args:\n        solution: The candidate solution to be evaluated. Its type\n            (e.g., `List[float]`, `np.ndarray`) must be consistent\n            with the `SolutionType` used in the class definition.\n\n    Returns:\n        An `Evaluation` object containing the fitness and constraint\n        violation information for the given solution.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/problem/#cilpy.problem.Problem.get_fitness_bounds","title":"<code>get_fitness_bounds()</code>","text":"<p>Returns the known theoretical min and max fitness values for the problem.</p> <p>This is used for calculating normalized performance metrics.</p> <p>Returns:</p> Type Description <code>Tuple[FitnessType, FitnessType]</code> <p>A tuple containing (global_minimum_fitness, global_maximum_fitness).</p> Source code in <code>cilpy/problem/__init__.py</code> <pre><code>def get_fitness_bounds(self) -&gt; Tuple[FitnessType, FitnessType]:\n    \"\"\"\n    Returns the known theoretical min and max fitness values for the\n    problem.\n\n    This is used for calculating normalized performance metrics.\n\n    Returns:\n        A tuple containing (global_minimum_fitness, global_maximum_fitness).\n    \"\"\"\n    raise NotImplementedError(\n        f\"The problem '{self.name}' does not have a known optimum value. \"\n        \"Implement get_fitness_bounds() to use metrics that require it.\"\n    )\n</code></pre>"},{"location":"api/problem/#cilpy.problem.Problem.is_dynamic","title":"<code>is_dynamic()</code>  <code>abstractmethod</code>","text":"<p>Indicates if the problem's landscape changes over time.</p> <p>Dynamic problems require specialized solvers that can adapt to changes in the objective function or constraints.</p> Note <p>Solvers can query this method to decide whether to employ change-detection mechanisms or re-evaluate their archives of best-so-far solutions.</p> <p>Returns:</p> Type Description <code>Tuple[bool, bool]</code> <p>A tuple <code>(is_objective_dynamic, is_constraint_dynamic)</code> where: - <code>is_objective_dynamic</code> is <code>True</code> if the objective   function(s) change over time. - <code>is_constraint_dynamic</code> is <code>True</code> if the constraint   function(s) change over time.</p> Source code in <code>cilpy/problem/__init__.py</code> <pre><code>@abstractmethod\ndef is_dynamic(self) -&gt; Tuple[bool, bool]:\n    \"\"\"Indicates if the problem's landscape changes over time.\n\n    Dynamic problems require specialized solvers that can adapt to\n    changes in the objective function or constraints.\n\n    Note:\n        Solvers can query this method to decide whether to employ\n        change-detection mechanisms or re-evaluate their archives of\n        best-so-far solutions.\n\n    Returns:\n        A tuple `(is_objective_dynamic, is_constraint_dynamic)` where:\n            - `is_objective_dynamic` is `True` if the objective\n              function(s) change over time.\n            - `is_constraint_dynamic` is `True` if the constraint\n              function(s) change over time.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/problem/#cilpy.problem.Problem.is_multi_objective","title":"<code>is_multi_objective()</code>  <code>abstractmethod</code>","text":"<p>Indicates if the problem has multiple objectives.</p> Source code in <code>cilpy/problem/__init__.py</code> <pre><code>@abstractmethod\ndef is_multi_objective(self) -&gt; bool:\n    \"\"\"Indicates if the problem has multiple objectives.\"\"\"\n    pass\n</code></pre>"},{"location":"api/runner/","title":"Runner","text":"<p>The experiment runner: Orchestrates computational intelligence experiments.</p> <p>This module provides the <code>ExperimentRunner</code> class, which is the primary tool for setting up, executing, and logging benchmark experiments in a structured and reproducible way.</p>"},{"location":"api/runner/#cilpy.runner.ExperimentRunner","title":"<code>ExperimentRunner</code>","text":"<p>Orchestrates the execution of computational intelligence experiments.</p> <p>This class is the main entry point for running experiments in <code>cilpy</code>. It automates the process of applying multiple solver configurations to a set of problems, handling independent runs, iteration loops, and results logging.</p> <p>The runner systematically pairs each solver with each problem, creating a dedicated output file for each combination. This declarative approach allows users to define complex experiments with minimal boilerplate code.</p> Example <p>.. code-block:: python</p> <pre><code>from cilpy.problem.unconstrained import Sphere\nfrom cilpy.solver.ga import GA\nfrom cilpy.runner import ExperimentRunner\n\n# 1. Define the problems to test on\nproblems = [Sphere(dimension=10)]\n\n# 2. Define the solver configurations to test\nsolver_configs = [\n    {\n        \"class\": GA,\n        \"params\": {\n            \"name\": \"GA_HighMutation\",\n            \"population_size\": 50,\n            \"mutation_rate\": 0.2,\n            \"crossover_rate\": 0.8,\n        }\n    },\n    {\n        \"class\": GA,\n        \"params\": {\n            \"name\": \"GA_LowMutation\",\n            \"population_size\": 50,\n            \"mutation_rate\": 0.05,\n            \"crossover_rate\": 0.8,\n        }\n    }\n]\n\n# 3. Initialize and run the experiment\nrunner = ExperimentRunner(\n    problems=problems,\n    solver_configurations=solver_configs,\n    num_runs=30,\n    max_iterations=1000\n)\nrunner.run_experiments()\n</code></pre> Source code in <code>cilpy/runner.py</code> <pre><code>class ExperimentRunner:\n    \"\"\"Orchestrates the execution of computational intelligence experiments.\n\n    This class is the main entry point for running experiments in `cilpy`. It\n    automates the process of applying multiple solver configurations to a set of\n    problems, handling independent runs, iteration loops, and results logging.\n\n    The runner systematically pairs each solver with each problem, creating a\n    dedicated output file for each combination. This declarative approach allows\n    users to define complex experiments with minimal boilerplate code.\n\n    Example:\n        .. code-block:: python\n\n            from cilpy.problem.unconstrained import Sphere\n            from cilpy.solver.ga import GA\n            from cilpy.runner import ExperimentRunner\n\n            # 1. Define the problems to test on\n            problems = [Sphere(dimension=10)]\n\n            # 2. Define the solver configurations to test\n            solver_configs = [\n                {\n                    \"class\": GA,\n                    \"params\": {\n                        \"name\": \"GA_HighMutation\",\n                        \"population_size\": 50,\n                        \"mutation_rate\": 0.2,\n                        \"crossover_rate\": 0.8,\n                    }\n                },\n                {\n                    \"class\": GA,\n                    \"params\": {\n                        \"name\": \"GA_LowMutation\",\n                        \"population_size\": 50,\n                        \"mutation_rate\": 0.05,\n                        \"crossover_rate\": 0.8,\n                    }\n                }\n            ]\n\n            # 3. Initialize and run the experiment\n            runner = ExperimentRunner(\n                problems=problems,\n                solver_configurations=solver_configs,\n                num_runs=30,\n                max_iterations=1000\n            )\n            runner.run_experiments()\n    \"\"\"\n\n    def __init__(\n        self,\n        problems: Sequence[Problem],\n        solver_configurations: List[Dict[str, Any]],\n        num_runs: int,\n        max_iterations: int,\n    ):\n        \"\"\"\n        Initializes the ExperimentRunner.\n\n        Args:\n            problems: A sequence of problem instances to be solved.\n                Each object must implement the `Problem` interface.\n            solver_configurations: A list of solver configurations.\n                Each configuration is a dictionary specifying the solver class\n                and its parameters. The `problem` parameter is injected\n                automatically by the runner and should not be included.\n            num_runs: The number of independent runs for each\n                solver-problem pair.\n            max_iterations: The number of iterations (`solver.step()` calls)\n                per run.\n\n        Solver Configuration Format:\n            .. code-block:: python\n\n                [\n                    {\n                        \"class\": YourSolverClass,\n                        \"params\": {\n                            \"name\": \"UniqueSolverName\",\n                            \"param1\": value1,\n                            # ... other solver hyperparameters\n                        }\n                    },\n                    # ... more configurations\n                ]\n        \"\"\"\n        self.problems = problems\n        self.solver_configurations = solver_configurations\n        self.num_runs = num_runs\n        self.max_iterations = max_iterations\n\n    def run_experiments(self):\n        \"\"\"Executes the full suite of defined experiments.\n\n        This method iterates through each problem and applies every configured\n        solver. For each problem-solver pair, it performs `num_runs` independent\n        runs, each lasting for `max_iterations`.\n\n        Results are logged to separate CSV files, with each file named using the\n        pattern: `{problem.name}_{solver_name}.out.csv`.\n        \"\"\"\n\n        total_start_time = time.time()\n        print(\"======== Starting All Experiments ========\")\n\n        # Ensure output directory exists\n        os.makedirs(\"out\", exist_ok=True)\n\n        for problem in self.problems:\n            print(f\"\\n--- Processing Problem: {problem.name} ---\")\n            for config in self.solver_configurations:\n                solver_class = config[\"class\"]\n                solver_params = config[\"params\"].copy()\n                constraint_handler_config = config.get(\"constraint_handler\")\n\n                # Add the current problem to the solver's parameters\n                current_solver_params = solver_params\n                current_solver_params[\"problem\"] = problem\n\n                solver_name = current_solver_params.get(\"name\")\n                output_file_path = os.path.join(\n                    \"out\", f\"{problem.name}_{solver_name}.out.csv\"\n                )\n\n                print(f\"\\n  -&gt; Starting Experiment: {solver_name} on {problem.name}\")\n                print(\n                    f\"     Configuration: {self.num_runs} runs, {self.max_iterations} iterations/run.\"\n                )\n                print(f\"     Results will be saved to: {output_file_path}\")\n\n                self._run_single_experiment(\n                    solver_class,\n                    current_solver_params,\n                    output_file_path,\n                    constraint_handler_config,\n                )\n\n        total_end_time = time.time()\n        print(\"\\n======== All Experiments Finished ========\")\n        print(f\"Total execution time: {total_end_time - total_start_time:.2f}s\")\n\n    def _is_solution_feasible(self, evaluation: Evaluation, tolerance=1e-6) -&gt; bool:\n        \"\"\"\n        Checks if an evaluation corresponds to a feasible solution.\n\n        A solution is feasible if all inequality constraints are &lt;= 0 and all\n        equality constraints are approximately == 0.\n\n        Args:\n            evaluation (Evaluation): The evaluation object to check.\n            tolerance (float): The tolerance for checking equality constraints.\n\n        Returns:\n            bool: True if the solution is feasible, False otherwise.\n        \"\"\"\n        if evaluation is None:\n            return False\n\n        # Check inequality constraints: g(x) &lt;= 0\n        if evaluation.constraints_inequality:\n            if any(v &gt; 0 for v in evaluation.constraints_inequality):\n                return False\n\n        # Check equality constraints: h(x) == 0\n        if evaluation.constraints_equality:\n            if any(abs(v) &gt; tolerance for v in evaluation.constraints_equality):\n                return False\n\n        return True\n\n    def _run_single_run(\n        self,\n        run_id: int,\n        constraint_handler_config: Optional[Dict],\n        solver_params: Dict,\n        solver_class: Type[Solver],\n        writer,\n        summary_file_path: str,\n    ):\n        run_start_time = time.time()\n        print(f\"     --- Starting Run {run_id}/{self.num_runs} ---\")\n\n        constraint_handler = None\n        if constraint_handler_config:\n            handler_class = constraint_handler_config[\"class\"]\n            handler_params = constraint_handler_config.get(\"params\", {})\n            constraint_handler = handler_class(**handler_params)\n\n        # Add the handler to the solver's parameters\n        current_solver_params = solver_params.copy()\n        current_solver_params[\"constraint_handler\"] = constraint_handler\n\n        # Re-instantiate the solver for each run to ensure independence\n        solver = solver_class(**current_solver_params)\n\n        # --- Safely get fitness bounds and set RE flag ---\n        bounds_known = False  # Flag to track if we can calculate relative error\n        f_max, fitness_range = 0, 0\n\n        try:\n            f_min, f_max = solver.problem.get_fitness_bounds()\n            fitness_range = f_max - f_min\n            if fitness_range &gt; 0:\n                bounds_known = True\n            else:\n                print(\n                    \"Warning: Fitness range is zero or invalid. Relative Error will not be calculated.\"\n                )\n        except NotImplementedError:\n            # The method is not implemented, so we leave bounds_known as False\n            print(\n                \"Info: get_fitness_bounds() not implemented for this problem. Skipping Relative Error.\"\n            )\n\n        # --- Run iterations ---\n        relative_error_history = []\n        for iteration in range(1, self.max_iterations + 1):\n            solver.problem.begin_iteration()\n            solver.step()\n            result = solver.get_result()\n\n            # --- Measure Accuracy and Relative Error ---\n            if solver.problem.is_multi_objective():\n                # --- Multi-Objective Case ---\n                accuracy = []  # This will hold the Pareto front\n                for objective in result:\n                    evaluation = objective[1]\n                    accuracy.append(evaluation.fitness)\n\n                # Relative Error is not applicable for a list of objectives\n                relative_error = \"\"\n            else:\n                # --- Single-Objective Case ---\n                # Get the single best fitness value\n                accuracy = result[0][1].fitness\n\n                # Calculate Relative Error, knowing 'accuracy' is a float\n                if bounds_known:\n                    # This is the correct formula for MINIMIZATION where a value\n                    # approaching f_min is better, and the result should approach 1.\n                    relative_error = (f_max - accuracy) / fitness_range\n                    relative_error_history.append(relative_error)\n                else:\n                    # If bounds are not known for this problem\n                    relative_error = \"\"\n\n            # --- Measure Feasibility ---\n            # Safely get population evaluations\n            try:\n                all_evaluations = solver.get_population_evaluations()\n                if all_evaluations:\n                    num_feasible = sum(\n                        1 for e in all_evaluations if self._is_solution_feasible(e)\n                    )\n                    feasibility = (num_feasible / len(all_evaluations)) * 100\n                else:\n                    feasibility = \"\"\n            except NotImplementedError:\n                # If the problem doesn't implement it, log empty strings\n                feasibility = \"\"\n\n            # --- Measure Diversity ---\n            # Safely get population\n            try:\n                population = solver.get_population()\n                diversity = 0.0\n                if population:\n                    pop_array = np.array(population)\n                    ns = pop_array.shape[0]\n\n                    # Calculate the centroid (mean vector) of the population\n                    centroid = np.mean(pop_array, axis=0)\n\n                    # Calculate the sum of squared Euclidean distances from the centroid\n                    sum_of_squared_diffs = np.sum((pop_array - centroid) ** 2)\n                    diversity = (1 / ns) * np.sqrt(sum_of_squared_diffs)\n            except NotImplementedError:\n                # If the problem doesn't implement it, log empty strings\n                diversity = \"\"\n\n            # Log the data for the current iteration\n            writer.writerow(\n                [run_id, iteration, accuracy, feasibility, diversity, relative_error]\n            )\n\n        # --- Calculate Relative Error Distance (P_RED) conditionally ---\n        if bounds_known:\n            b_vector = np.array(relative_error_history)\n            nv = len(b_vector)\n            sum_of_squares = np.sum((1 - b_vector) ** 2)\n            relative_error_distance = (\n                np.sqrt(sum_of_squares) / np.sqrt(nv) if nv &gt; 0 else 0.0\n            )\n        else:\n            # If we couldn't calculate P_RE, we can't calculate P_RED either\n            relative_error_distance = \"\"\n\n        run_end_time = time.time()\n        final_result = solver.get_result()\n        print(\n            f\"     Run {run_id} finished in {run_end_time - run_start_time:.2f}s. \"\n            f\"Best fitness: {final_result[0][1].fitness if final_result else 'N/A'}\"\n        )\n\n        # --- Log the final P_RED for the entire run ---\n        red_output = (\n            f\"{relative_error_distance:.6f}\"\n            if isinstance(relative_error_distance, float)\n            else \"N/A (bounds unknown)\"\n        )\n        print(f\"     Relative Error Distance (P_RED) for Run {run_id}: {red_output}\")\n\n        with open(summary_file_path, \"a\", newline=\"\") as f:\n            summary_writer = csv.writer(f)\n            problem_name = solver.problem.name\n            solver_name = solver.name\n            summary_writer.writerow(\n                [problem_name, solver_name, run_id, relative_error_distance]\n            )\n\n    def _run_single_experiment(\n        self,\n        solver_class: Type[Solver],\n        solver_params: Dict,\n        output_file: str,\n        constraint_handler_config: Optional[Dict] = None,\n    ):\n        \"\"\"Runs and logs a single experiment for a given solver on a problem.\n\n        This internal method is called by `run_experiments`. It handles the\n        instantiation of the solver for each of the `num_runs` and manages the\n        iteration loop and CSV writing for a single problem-solver pair.\n\n        The output CSV file contains the following columns:\n        - `run_id`: The ID of the independent run (from 1 to `num_runs`).\n        - `iteration`: The current iteration number (from 1 to `max_iterations`).\n        - `accuracy`: Fitness of best solution(s) found so far.\n        - `feasibility`: The percentage of solutions that are feasible.\n        - `diversity`: A measure of the diversity at this iteration.\n        - `relative_error`: The relative error at this iteration.\n\n        Args:\n            solver_class: The solver class to be instantiated.\n            solver_params: The parameters for initializing the solver (including\n                the `problem` instance).\n            output_file: The path to the output CSV file.\n        \"\"\"\n        # Prepare output files\n        summary_file_path = output_file.replace(\".out.csv\", \".summary.out.csv\")\n        main_header = [\n            \"run_id\",\n            \"iteration\",\n            \"accuracy\",\n            \"feasibility\",\n            \"diversity\",\n            \"relative_error\",\n        ]\n        summary_header = [\n            \"problem_name\",\n            \"solver_name\",\n            \"run_id\",\n            \"relative_error_distance\",\n        ]\n\n        with open(summary_file_path, \"w\", newline=\"\") as f_summary:\n            summary_writer = csv.writer(f_summary)\n            summary_writer.writerow(summary_header)\n\n        experiment_start_time = time.time()\n\n        with open(output_file, \"w\", newline=\"\") as f_main:\n            writer = csv.writer(f_main)\n            writer.writerow(main_header)\n\n            # Run experiments\n            for run_id in range(1, self.num_runs + 1):\n                self._run_single_run(\n                    run_id,\n                    constraint_handler_config,\n                    solver_params,\n                    solver_class,\n                    writer,\n                    summary_file_path,\n                )\n\n        experiment_end_time = time.time()\n        solver_name = solver_params.get(\"name\", solver_class.__name__)\n        problem_name = solver_params[\"problem\"].name\n        print(\n            f\"  -&gt; Experiment for {solver_name} on {problem_name} \"\n            f\"finished in {experiment_end_time - experiment_start_time:.2f}s.\"\n        )\n        print(f\"     Summary results saved to: {summary_file_path}\")\n</code></pre>"},{"location":"api/runner/#cilpy.runner.ExperimentRunner.__init__","title":"<code>__init__(problems, solver_configurations, num_runs, max_iterations)</code>","text":"<p>Initializes the ExperimentRunner.</p> <p>Parameters:</p> Name Type Description Default <code>problems</code> <code>Sequence[Problem]</code> <p>A sequence of problem instances to be solved. Each object must implement the <code>Problem</code> interface.</p> required <code>solver_configurations</code> <code>List[Dict[str, Any]]</code> <p>A list of solver configurations. Each configuration is a dictionary specifying the solver class and its parameters. The <code>problem</code> parameter is injected automatically by the runner and should not be included.</p> required <code>num_runs</code> <code>int</code> <p>The number of independent runs for each solver-problem pair.</p> required <code>max_iterations</code> <code>int</code> <p>The number of iterations (<code>solver.step()</code> calls) per run.</p> required Solver Configuration Format <p>.. code-block:: python</p> <pre><code>[\n    {\n        \"class\": YourSolverClass,\n        \"params\": {\n            \"name\": \"UniqueSolverName\",\n            \"param1\": value1,\n            # ... other solver hyperparameters\n        }\n    },\n    # ... more configurations\n]\n</code></pre> Source code in <code>cilpy/runner.py</code> <pre><code>def __init__(\n    self,\n    problems: Sequence[Problem],\n    solver_configurations: List[Dict[str, Any]],\n    num_runs: int,\n    max_iterations: int,\n):\n    \"\"\"\n    Initializes the ExperimentRunner.\n\n    Args:\n        problems: A sequence of problem instances to be solved.\n            Each object must implement the `Problem` interface.\n        solver_configurations: A list of solver configurations.\n            Each configuration is a dictionary specifying the solver class\n            and its parameters. The `problem` parameter is injected\n            automatically by the runner and should not be included.\n        num_runs: The number of independent runs for each\n            solver-problem pair.\n        max_iterations: The number of iterations (`solver.step()` calls)\n            per run.\n\n    Solver Configuration Format:\n        .. code-block:: python\n\n            [\n                {\n                    \"class\": YourSolverClass,\n                    \"params\": {\n                        \"name\": \"UniqueSolverName\",\n                        \"param1\": value1,\n                        # ... other solver hyperparameters\n                    }\n                },\n                # ... more configurations\n            ]\n    \"\"\"\n    self.problems = problems\n    self.solver_configurations = solver_configurations\n    self.num_runs = num_runs\n    self.max_iterations = max_iterations\n</code></pre>"},{"location":"api/runner/#cilpy.runner.ExperimentRunner.run_experiments","title":"<code>run_experiments()</code>","text":"<p>Executes the full suite of defined experiments.</p> <p>This method iterates through each problem and applies every configured solver. For each problem-solver pair, it performs <code>num_runs</code> independent runs, each lasting for <code>max_iterations</code>.</p> <p>Results are logged to separate CSV files, with each file named using the pattern: <code>{problem.name}_{solver_name}.out.csv</code>.</p> Source code in <code>cilpy/runner.py</code> <pre><code>def run_experiments(self):\n    \"\"\"Executes the full suite of defined experiments.\n\n    This method iterates through each problem and applies every configured\n    solver. For each problem-solver pair, it performs `num_runs` independent\n    runs, each lasting for `max_iterations`.\n\n    Results are logged to separate CSV files, with each file named using the\n    pattern: `{problem.name}_{solver_name}.out.csv`.\n    \"\"\"\n\n    total_start_time = time.time()\n    print(\"======== Starting All Experiments ========\")\n\n    # Ensure output directory exists\n    os.makedirs(\"out\", exist_ok=True)\n\n    for problem in self.problems:\n        print(f\"\\n--- Processing Problem: {problem.name} ---\")\n        for config in self.solver_configurations:\n            solver_class = config[\"class\"]\n            solver_params = config[\"params\"].copy()\n            constraint_handler_config = config.get(\"constraint_handler\")\n\n            # Add the current problem to the solver's parameters\n            current_solver_params = solver_params\n            current_solver_params[\"problem\"] = problem\n\n            solver_name = current_solver_params.get(\"name\")\n            output_file_path = os.path.join(\n                \"out\", f\"{problem.name}_{solver_name}.out.csv\"\n            )\n\n            print(f\"\\n  -&gt; Starting Experiment: {solver_name} on {problem.name}\")\n            print(\n                f\"     Configuration: {self.num_runs} runs, {self.max_iterations} iterations/run.\"\n            )\n            print(f\"     Results will be saved to: {output_file_path}\")\n\n            self._run_single_experiment(\n                solver_class,\n                current_solver_params,\n                output_file_path,\n                constraint_handler_config,\n            )\n\n    total_end_time = time.time()\n    print(\"\\n======== All Experiments Finished ========\")\n    print(f\"Total execution time: {total_end_time - total_start_time:.2f}s\")\n</code></pre>"},{"location":"api/solver/","title":"<code>cilpy.solver</code> Documentation","text":"<p>The solver module: Defines the optimization algorithm interface.</p> <p>This module provides the abstract \"contract\" for all optimization algorithms (solvers) within the <code>cilpy</code> library.</p> <p>The core component is the <code>Solver</code> abstract base class. Any algorithm that inherits from this class and implements its methods can be used by the <code>ExperimentRunner</code> to solve any <code>cilpy</code> problem.</p>"},{"location":"api/solver/#cilpy.solver.Solver","title":"<code>Solver</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[SolutionType, FitnessType]</code></p> <p>An abstract interface for a problem solver.</p> <p>This class is the blueprint for all optimization algorithms in <code>cilpy</code>. To create a new solver, inherit from this class and implement its abstract methods. The interface is generic to support different solution types (e.g., <code>List[float]</code>, <code>np.ndarray</code>) and fitness structures.</p> <p>Attributes:</p> Name Type Description <code>problem</code> <code>Problem</code> <p>The problem instance that the solver is optimizing.</p> <code>name</code> <code>str</code> <p>The name of the solver instance.</p> <code>comparator</code> <code>ConstraintHandler</code> <p>The constraint-handling comparator used to comparesolutions.</p> Example <p>A minimal implementation for a Random Search solver.</p> <p>.. code-block:: python</p> <pre><code>import random\nfrom cilpy.problem import Problem, Evaluation\nfrom cilpy.solver import Solver\n\nclass RandomSearch(Solver[list[float], float]):\n    def __init__(self, problem: Problem, name: str):\n        super().__init__(problem, name)\n        self.best_solution = None\n        self.best_eval = Evaluation(fitness=float('inf'))\n\n    def step(self) -&gt; None:\n        # Generate one random solution\n        lower, upper = self.problem.bounds\n        solution = [random.uniform(l, u) for l, u in zip(lower, upper)]\n        evaluation = self.problem.evaluate(solution)\n\n        # Update best if necessary\n        if evaluation.fitness &lt; self.best_eval.fitness:\n            self.best_solution = solution\n            self.best_eval = evaluation\n\n    def get_result(self) -&gt; list[tuple[list[float], Evaluation[float]]]:\n        return [(self.best_solution, self.best_eval)]\n</code></pre> Source code in <code>cilpy/solver/__init__.py</code> <pre><code>class Solver(ABC, Generic[SolutionType, FitnessType]):\n    \"\"\"An abstract interface for a problem solver.\n\n    This class is the blueprint for all optimization algorithms in `cilpy`. To\n    create a new solver, inherit from this class and implement its abstract\n    methods. The interface is generic to support different solution types\n    (e.g., `List[float]`, `np.ndarray`) and fitness structures.\n\n    Attributes:\n        problem (cilpy.problem.Problem): The problem instance that the solver is\n            optimizing.\n        name (str): The name of the solver instance.\n        comparator (cilpy.solver.chm.ConstraintHandler): The constraint-handling\n            comparator used to comparesolutions.\n\n    Example:\n        A minimal implementation for a Random Search solver.\n\n        .. code-block:: python\n\n            import random\n            from cilpy.problem import Problem, Evaluation\n            from cilpy.solver import Solver\n\n            class RandomSearch(Solver[list[float], float]):\n                def __init__(self, problem: Problem, name: str):\n                    super().__init__(problem, name)\n                    self.best_solution = None\n                    self.best_eval = Evaluation(fitness=float('inf'))\n\n                def step(self) -&gt; None:\n                    # Generate one random solution\n                    lower, upper = self.problem.bounds\n                    solution = [random.uniform(l, u) for l, u in zip(lower, upper)]\n                    evaluation = self.problem.evaluate(solution)\n\n                    # Update best if necessary\n                    if evaluation.fitness &lt; self.best_eval.fitness:\n                        self.best_solution = solution\n                        self.best_eval = evaluation\n\n                def get_result(self) -&gt; list[tuple[list[float], Evaluation[float]]]:\n                    return [(self.best_solution, self.best_eval)]\n    \"\"\"\n\n    @abstractmethod\n    def __init__(\n        self,\n        problem: Problem[SolutionType, FitnessType],\n        name: str,\n        constraint_handler: Optional[ConstraintHandler[FitnessType]] = None,\n        **kwargs,\n    ):\n        \"\"\"Initializes the solver.\n\n        Subclasses must call `super().__init__(...)` and can use `**kwargs` to\n        accept algorithm-specific hyperparameters.\n\n        Args:\n            problem: The optimization problem to solve, which must\n                implement the `Problem` interface.\n            name: The name of the solver instance.\n            constraint_handler: An optional strategy object for handling\n                constraints. If None, a default fitness-only comparator is used.\n            **kwargs: A dictionary for algorithm-specific parameters. For\n                example, a GA might accept `population_size=100` or\n                `mutation_rate=0.1`.\n        \"\"\"\n        self.problem = problem\n        self.name = name\n        self.comparator = constraint_handler or DefaultComparator()\n\n    @abstractmethod\n    def step(self) -&gt; None:\n        \"\"\"Performs one iteration of the optimization algorithm.\n\n        This method contains the core logic of the solver. The\n        `ExperimentRunner` will call this method repeatedly in a loop. A single\n        step could be one generation in a GA, one iteration in a PSO, or the\n        evaluation of one new solution in a simpler algorithm.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_result(self) -&gt; List[Tuple[SolutionType, Evaluation[FitnessType]]]:\n        \"\"\"Returns the best solution(s) found so far.\n\n        This method provides the current result of the optimization process. It\n        is called by the `ExperimentRunner` after each step to log progress.\n\n        Returns:\n            A list of tuples, where each tuple contains `(solution,\n            evaluation)`.\n            - For single-objective solvers, this list should contain a single\n              tuple with the best solution found.\n            - For multi-objective solvers, this list should contain the set\n              of non-dominated solutions (the Pareto front archive).\n\n            Example return for a single-objective solver:\n            `[([0.1, -0.5], Evaluation(fitness=0.26))]`\n        \"\"\"\n        pass\n\n    def get_population(self) -&gt; List[SolutionType]:\n        \"\"\"\n        Returns the entire current population or set of candidate solutions.\n\n        This method is optional and should be implemented by population-based\n        algorithms. It is required for certain performance metrics like\n        diversity measurement.\n\n        Raises:\n            NotImplementedError: If the solver is not swarm based.\n\n        Returns:\n            All individuals in the solver's population.\n        \"\"\"\n        raise NotImplementedError(\n            f\"\"\"The solver '{self.name}' does not have a population. Implement\n             get_population() to use metrics that require it.\"\"\"\n        )\n\n    def get_population_evaluations(self) -&gt; List[Evaluation[FitnessType]]:\n        \"\"\"\n        Returns the evaluations of the entire current population or set of\n        candidate solutions.\n\n        This method is optional and should be implemented by population-based\n        algorithms. It is required for certain performance metrics like\n        percentage of feasible solutions.\n\n        Raises:\n            NotImplementedError: If the solver is not swarm based.\n\n        Returns:\n            Evaluations for all individuals in the solver's population.\n        \"\"\"\n        raise NotImplementedError(\n            f\"\"\"The solver '{self.name}' does not have a population. Implement\n             get_population_evaluations() to use metrics that require it.\"\"\"\n        )\n</code></pre>"},{"location":"api/solver/#cilpy.solver.Solver.__init__","title":"<code>__init__(problem, name, constraint_handler=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Initializes the solver.</p> <p>Subclasses must call <code>super().__init__(...)</code> and can use <code>**kwargs</code> to accept algorithm-specific hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>Problem[SolutionType, FitnessType]</code> <p>The optimization problem to solve, which must implement the <code>Problem</code> interface.</p> required <code>name</code> <code>str</code> <p>The name of the solver instance.</p> required <code>constraint_handler</code> <code>Optional[ConstraintHandler[FitnessType]]</code> <p>An optional strategy object for handling constraints. If None, a default fitness-only comparator is used.</p> <code>None</code> <code>**kwargs</code> <p>A dictionary for algorithm-specific parameters. For example, a GA might accept <code>population_size=100</code> or <code>mutation_rate=0.1</code>.</p> <code>{}</code> Source code in <code>cilpy/solver/__init__.py</code> <pre><code>@abstractmethod\ndef __init__(\n    self,\n    problem: Problem[SolutionType, FitnessType],\n    name: str,\n    constraint_handler: Optional[ConstraintHandler[FitnessType]] = None,\n    **kwargs,\n):\n    \"\"\"Initializes the solver.\n\n    Subclasses must call `super().__init__(...)` and can use `**kwargs` to\n    accept algorithm-specific hyperparameters.\n\n    Args:\n        problem: The optimization problem to solve, which must\n            implement the `Problem` interface.\n        name: The name of the solver instance.\n        constraint_handler: An optional strategy object for handling\n            constraints. If None, a default fitness-only comparator is used.\n        **kwargs: A dictionary for algorithm-specific parameters. For\n            example, a GA might accept `population_size=100` or\n            `mutation_rate=0.1`.\n    \"\"\"\n    self.problem = problem\n    self.name = name\n    self.comparator = constraint_handler or DefaultComparator()\n</code></pre>"},{"location":"api/solver/#cilpy.solver.Solver.get_population","title":"<code>get_population()</code>","text":"<p>Returns the entire current population or set of candidate solutions.</p> <p>This method is optional and should be implemented by population-based algorithms. It is required for certain performance metrics like diversity measurement.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the solver is not swarm based.</p> <p>Returns:</p> Type Description <code>List[SolutionType]</code> <p>All individuals in the solver's population.</p> Source code in <code>cilpy/solver/__init__.py</code> <pre><code>def get_population(self) -&gt; List[SolutionType]:\n    \"\"\"\n    Returns the entire current population or set of candidate solutions.\n\n    This method is optional and should be implemented by population-based\n    algorithms. It is required for certain performance metrics like\n    diversity measurement.\n\n    Raises:\n        NotImplementedError: If the solver is not swarm based.\n\n    Returns:\n        All individuals in the solver's population.\n    \"\"\"\n    raise NotImplementedError(\n        f\"\"\"The solver '{self.name}' does not have a population. Implement\n         get_population() to use metrics that require it.\"\"\"\n    )\n</code></pre>"},{"location":"api/solver/#cilpy.solver.Solver.get_population_evaluations","title":"<code>get_population_evaluations()</code>","text":"<p>Returns the evaluations of the entire current population or set of candidate solutions.</p> <p>This method is optional and should be implemented by population-based algorithms. It is required for certain performance metrics like percentage of feasible solutions.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the solver is not swarm based.</p> <p>Returns:</p> Type Description <code>List[Evaluation[FitnessType]]</code> <p>Evaluations for all individuals in the solver's population.</p> Source code in <code>cilpy/solver/__init__.py</code> <pre><code>def get_population_evaluations(self) -&gt; List[Evaluation[FitnessType]]:\n    \"\"\"\n    Returns the evaluations of the entire current population or set of\n    candidate solutions.\n\n    This method is optional and should be implemented by population-based\n    algorithms. It is required for certain performance metrics like\n    percentage of feasible solutions.\n\n    Raises:\n        NotImplementedError: If the solver is not swarm based.\n\n    Returns:\n        Evaluations for all individuals in the solver's population.\n    \"\"\"\n    raise NotImplementedError(\n        f\"\"\"The solver '{self.name}' does not have a population. Implement\n         get_population_evaluations() to use metrics that require it.\"\"\"\n    )\n</code></pre>"},{"location":"api/solver/#cilpy.solver.Solver.get_result","title":"<code>get_result()</code>  <code>abstractmethod</code>","text":"<p>Returns the best solution(s) found so far.</p> <p>This method provides the current result of the optimization process. It is called by the <code>ExperimentRunner</code> after each step to log progress.</p> <p>Returns:</p> Type Description <code>List[Tuple[SolutionType, Evaluation[FitnessType]]]</code> <p>A list of tuples, where each tuple contains `(solution,</p> <code>List[Tuple[SolutionType, Evaluation[FitnessType]]]</code> <p>evaluation)`.</p> <code>List[Tuple[SolutionType, Evaluation[FitnessType]]]</code> <ul> <li>For single-objective solvers, this list should contain a single tuple with the best solution found.</li> </ul> <code>List[Tuple[SolutionType, Evaluation[FitnessType]]]</code> <ul> <li>For multi-objective solvers, this list should contain the set of non-dominated solutions (the Pareto front archive).</li> </ul> <code>List[Tuple[SolutionType, Evaluation[FitnessType]]]</code> <p>Example return for a single-objective solver:</p> <code>List[Tuple[SolutionType, Evaluation[FitnessType]]]</code> <p><code>[([0.1, -0.5], Evaluation(fitness=0.26))]</code></p> Source code in <code>cilpy/solver/__init__.py</code> <pre><code>@abstractmethod\ndef get_result(self) -&gt; List[Tuple[SolutionType, Evaluation[FitnessType]]]:\n    \"\"\"Returns the best solution(s) found so far.\n\n    This method provides the current result of the optimization process. It\n    is called by the `ExperimentRunner` after each step to log progress.\n\n    Returns:\n        A list of tuples, where each tuple contains `(solution,\n        evaluation)`.\n        - For single-objective solvers, this list should contain a single\n          tuple with the best solution found.\n        - For multi-objective solvers, this list should contain the set\n          of non-dominated solutions (the Pareto front archive).\n\n        Example return for a single-objective solver:\n        `[([0.1, -0.5], Evaluation(fitness=0.26))]`\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/solver/#cilpy.solver.Solver.step","title":"<code>step()</code>  <code>abstractmethod</code>","text":"<p>Performs one iteration of the optimization algorithm.</p> <p>This method contains the core logic of the solver. The <code>ExperimentRunner</code> will call this method repeatedly in a loop. A single step could be one generation in a GA, one iteration in a PSO, or the evaluation of one new solution in a simpler algorithm.</p> Source code in <code>cilpy/solver/__init__.py</code> <pre><code>@abstractmethod\ndef step(self) -&gt; None:\n    \"\"\"Performs one iteration of the optimization algorithm.\n\n    This method contains the core logic of the solver. The\n    `ExperimentRunner` will call this method repeatedly in a loop. A single\n    step could be one generation in a GA, one iteration in a PSO, or the\n    evaluation of one new solution in a simpler algorithm.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dev/","title":"Developer Guide","text":"<p>Welcome to the <code>cilpy</code> developer guide! We're excited that you're interested in contributing. This document provides everything you need to know to get your development environment set up and contribute to the project effectively.</p> <p>Our goal is to create a library that is powerful yet easy to maintain and extend. Following these guidelines helps us achieve that.</p>"},{"location":"dev/#1-setting-up-your-development-environment","title":"1. Setting Up Your Development Environment","text":"<p>To contribute code, you first need to set up a local development environment.</p> <p>1. Fork and Clone the Repository: First, fork the repository on the GitLab instance. Then, clone your fork to your local machine:  </p> <pre><code>git clone git@git.cs.sun.ac.za:Computer-Science/rw771/2025/24717274-AE4-src.git  \ncd 24717274-AE4-src\n</code></pre> <p>2. Install in Editable Mode: Installing the package in \"editable\" mode allows you to test your changes live without having to reinstall the package after every modification.</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"dev/#2-core-design-philosophy","title":"2. Core Design Philosophy","text":"<p><code>cilpy</code> is built on three core principles. When developing new features, please keep these in mind:</p> <ul> <li>Genericity: The library's interfaces should be abstract enough to     support a wide variety of CI paradigms, from single-objective GAs to multi-     population co-evolutionary systems. Avoid making assumptions that would     limit the framework to a specific type of problem or solver.</li> <li>Extendability: The primary goal is to make it easy for researchers to     add their own components. This is achieved through the core <code>Problem</code> and     <code>Solver</code> abstract base classes. A user should be able to add a new algorithm     and immediately test it on all existing problems, and vice-versa.</li> <li>Maintainability: Clean, readable, and well-documented code is essential.     We enforce this through mandatory type hinting, adherence to style guides,     and a requirement for unit tests.</li> </ul>"},{"location":"dev/#3-the-contribution-workflow","title":"3. The Contribution Workflow","text":"<p>We follow a standard fork-and-pull-request workflow.</p> <ol> <li>Find an Issue: Start by looking at the To-Do List or the     issue tracker for tasks. If you have a new idea, consider creating an issue     first to discuss it with the maintainers.</li> <li>Create a Branch: From the <code>main</code> branch, create a new feature branch for     your work. Please use the branch prefixes outlined below.     <code>bash     # Example for a new feature     git checkout -b ft/add-new-pso-variant main</code></li> <li>Write Code: Make your changes, following the coding style guidelines     below.</li> <li>Write Tests: All new features must be accompanied by tests. If you add a     new problem, add a test case for it in <code>test/test_problems.py</code>. If you add a     new solver, create a test for it. Tests are crucial for ensuring the long-     term stability of the library. Learn more about our testing strategy     here</li> <li>Ensure All Tests Pass: Run the test suite to make sure your changes     haven't broken existing functionality.</li> <li>Submit a Merge Request: Push your branch to your fork and open a Merge     Request to the main <code>cilpy</code> repository. Provide a clear description of the     changes you have made.</li> </ol>"},{"location":"dev/#branch-naming-prefixes","title":"Branch Naming Prefixes","text":"<ul> <li><code>ft/</code>: Feature development (e.g., <code>ft/quantum-pso</code>)</li> <li><code>fx/</code>: Bug fixes (e.g., <code>fx/fix-ga-selection-bug</code>)</li> <li><code>ch/</code>: Maintenance tasks (e.g., <code>ch/update-dependencies</code>)</li> <li><code>rf/</code>: Code refactoring (e.g., <code>rf/optimize-runner-loop</code>)</li> <li><code>dc/</code>: Documentation updates (e.g., <code>dc/clarify-solver-api</code>)</li> <li><code>ts/</code>: Testing-related work (e.g., <code>ts/add-tests-for-mpb</code>)</li> </ul>"},{"location":"dev/#4-how-to-add-new-components","title":"4. How to Add New Components","text":"<p>This is the most common way to contribute. The key is to correctly implement the required interface.</p>"},{"location":"dev/#adding-a-new-problem","title":"Adding a New Problem","text":"<ol> <li>Create the File: Add a new Python file in the <code>cilpy/problem/</code>     directory.</li> <li>Implement the Interface: Your new class must inherit from     <code>cilpy.problem.Problem</code> and implement all of its abstract methods.</li> <li>Write a Test: Add a test case to <code>test/test_problems.py</code> that     instantiates your problem and evaluates a known solution to ensure the     fitness calculation is correct.</li> </ol> <p>Template for a new problem:</p> <pre><code># In cilpy/problem/my_new_problem.py\nfrom typing import List, Tuple\nfrom cilpy.problem import Problem, Evaluation\n\nclass MyNewProblem(Problem[List[float], float]):\n    def __init__(self, dimension: int):\n        # Always call super().__init__()\n        super().__init__(\n            dimension=dimension,\n            bounds=([-10.0] * dimension, [10.0] * dimension),\n            name=\"MyNewProblem\"\n        )\n\n    def evaluate(self, solution: List[float]) -&gt; Evaluation[float]:\n        # Your fitness logic here\n        fitness = ...\n        return Evaluation(fitness=fitness)\n\n    def is_dynamic(self) -&gt; Tuple[bool, bool]:\n        # Return True for the first element if the objective is dynamic\n        return (False, False)\n</code></pre>"},{"location":"dev/#adding-a-new-solver","title":"Adding a New Solver","text":"<ol> <li>Create the File: Add a new Python file in the <code>cilpy/solver/solvers/</code>     directory.</li> <li>Implement the Interface: Your new class must inherit from     <code>cilpy.solver.Solver</code> and implement all of its abstract methods.</li> <li>Write a Test: Add a test that runs your solver on a simple, known     problem (like Sphere) and asserts that it finds a reasonably good solution.</li> </ol> <p>Template for a new solver:</p> <pre><code># In cilpy/solver/solvers/my_new_solver.py\nfrom typing import List, Tuple\nfrom cilpy.problem import Problem, Evaluation\nfrom cilpy.solver import Solver\n\nclass MyNewSolver(Solver[List[float], float]):\n    def __init__(self, problem: Problem, name: str, **kwargs):\n        # Always call super().__init__()\n        super().__init__(problem, name)\n        # Your initialization logic here (e.g., population)\n        self.best_solution = None\n        self.best_evaluation = Evaluation(fitness=float('inf'))\n\n    def step(self) -&gt; None:\n        # Your core algorithm logic for one iteration\n        # ... update self.best_solution and self.best_evaluation\n        pass\n\n    def get_result(self) -&gt; List[Tuple[List[float], Evaluation[float]]]:\n        # Return the best result found so far\n        return [(self.best_solution, self.best_evaluation)]\n</code></pre>"},{"location":"dev/#5-coding-style-and-conventions","title":"5. Coding Style and Conventions","text":"<p>Consistency is key. Please adhere to the following standards.</p> <ul> <li>Docstrings: All public modules, classes, and functions must have     Google-style docstrings.</li> <li>Style Guide: All code must adhere to PEP 8.     We recommend using an autoformatter like <code>black</code>.</li> <li>Line Length: Maximum 80 characters for code and docstrings.</li> <li>Type Hinting: All function and method signatures must include type     hints.</li> <li>Imports: Group imports in this order: (1) Python standard library, (2)     third-party libraries, (3) <code>cilpy</code> imports.</li> <li>Dependencies: The core library should have minimal dependencies. Please     discuss with a maintainer before adding a new third-party dependency.</li> </ul>"},{"location":"dev/#where-to-start","title":"Where to Start?","text":"<p>A great place to start contributing is by looking at our To-Do List or picking up an unassigned issue from the issue tracker. We look forward to your contributions!</p>"},{"location":"dev/testing/","title":"Testing","text":""},{"location":"dev/testing/#testing-strategy","title":"Testing Strategy","text":"<p>The <code>cilpy</code> library is developed with a strong emphasis on correctness and reliability, which is maintained through a comprehensive suite of unit tests. Our testing strategy ensures that every component of the library is independently verifiable and that new contributions can be integrated with confidence.</p>"},{"location":"dev/testing/#how-testing-works","title":"How Testing Works","text":"<p>Our testing is built on a few core principles and tools:</p> <ul> <li>Framework: We use the <code>pytest</code> framework for writing and running tests.     <code>pytest</code>'s test discovery mechanism automatically finds and runs all tests     located in files named <code>test_*.py</code>.</li> <li>Isolation and Mocking: To test a component's logic without interference     from its dependencies, we heavily utilize mocking via Python's     <code>unittest.mock</code> library. For example, when testing a <code>Solver</code>, the <code>Problem</code>     it is trying to solve is replaced with a \"mock\" object. This allows us to     control the exact fitness values returned for any given solution, enabling     us to create predictable scenarios and verify that the solver's internal     logic (e.g., selection, elitism) behaves as expected.</li> <li>Generic Design: A key feature of the test suite is its generic and     extensible nature. For components like <code>Problems</code> and <code>Constraint Handlers</code>,     we use <code>pytest.mark.parametrize</code> to define a single set of tests that     automatically runs against every new implementation. This means that when a     developer adds a new benchmark function, they only need to add it to a list     in the test file to have it fully validated against the library's interface     contract.</li> </ul>"},{"location":"dev/testing/#test-coverage","title":"Test Coverage","text":"<p>Our unit tests are organized to mirror the library's structure, providing coverage for each of its core components:</p> <ol> <li> <p>Problems (<code>cilpy.problem</code>)</p> <ul> <li>Initialization: Verifies that all problems are instantiated with the     correct <code>dimension</code>, <code>bounds</code>, and <code>name</code>.</li> <li>Evaluation: Confirms that the <code>evaluate</code> method returns an     <code>Evaluation</code> object with the correct structure and data types for     fitness and constraints.</li> <li>Dynamic Behavior: For dynamic problems like the Moving Peaks     Benchmark (MPB and CMPB), tests confirm that the landscape changes are     triggered correctly based on the evaluation count.</li> </ul> </li> <li> <p>Constraint Handling Mechanisms (<code>cilpy.solver.chm</code>)</p> <ul> <li>Initialization: Ensures that handlers are created with valid     parameters (e.g., <code>alpha</code> in <code>AlphaConstraintHandler</code> must be in the     range).</li> <li>Comparison Logic: Rigorously tests the <code>is_better</code> method for all     possible comparison scenarios. For example, the <code>AlphaConstraintHandler</code>     tests cover cases where both solutions are feasible, only one is     feasible, or their satisfaction levels are equal.</li> <li>Internal Calculations: Validates helper functions, such as the     <code>_calculate_satisfaction</code> method in the <code>AlphaConstraintHandler</code>,     against their formal definitions.</li> </ul> </li> <li> <p>Solvers (<code>cilpy.solver</code>)</p> <ul> <li>Initialization: Checks that solvers correctly set up their initial     state and generate a valid initial population.</li> <li>Algorithmic Operators: Each core mechanism of a solver is tested in     isolation using mocked problems. This includes:<ul> <li>GA: Tournament selection, single-point crossover, Gaussian     mutation, and elitism.</li> <li>RIGA: The replacement of the worst individuals with random     immigrants.</li> <li>HyperMGA: The state-switching logic that toggles between normal     and hyper-mutation modes based on environmental changes.</li> </ul> </li> <li>Result Retrieval: Verifies that the <code>get_result</code> method correctly     identifies and returns the best solution from the current population     based on the active <code>comparator</code>.</li> </ul> </li> </ol> <p>To run the entire test suite, simply execute the following command from the root directory of the project:</p> <pre><code>pytest\n</code></pre>"},{"location":"dev/todo/","title":"Todo","text":"<p>Extend the library so that the <code>compare</code> component has all the features of this stats tool.</p> <p>Write unit tests.</p> <p>Write integration tests.</p> <p>Consider how the library could interact with libraries used for:</p> <ul> <li>[ ] empirical analysis</li> <li>[ ] fitness landscape analysis</li> <li>[ ] results repositories</li> </ul> <p>Ensure the library enables the following constraint handling techniques:</p> <ul> <li>[ ] techniques ensuring feasibility of solutions throughout the search   process.</li> <li>[ ] techniques allowing infeasible solutions during the search process, while   applying repair mechanisms later.</li> <li>[X] techniques which formulate the constrained optimization problem as a   box-constrained optimization problem through the use of penalty methods.</li> <li>[X] techniques which formulate the constrained optimization problem as a dual   Lagrangian.</li> <li>[ ] techniques which formulate the constrained optimization problem as a   box-constrained multi-/many-objective optimization problem, and then to use   multi-/many-objective optimization problem to find feasible solutions.</li> </ul>"},{"location":"lib/","title":"Overview","text":"<p>These documents describe the problems and solvers included in the library.</p>"},{"location":"lib/compare/","title":"Compare","text":"<p>The goal of this component is that it should allow users to conduct statistical tests to compare algorithms, much like this tool.</p>"},{"location":"lib/problem/","title":"Included Problems","text":"<p>The Moving Peaks Benchmark (MPB) for dynamic optimization problems.</p> <p>This module provides an implementation of the Moving Peaks Benchmark (MPB) generator, a widely used tool for creating dynamic, multi-peaked optimization landscapes. It is designed to test the ability of optimization algorithms to adapt to changing environments.</p> <p>The MPB landscape is defined by a number of peaks, each with its own height, width, and position. At specified intervals (controlled by <code>change_frequency</code>), these peak properties are updated, causing the landscape to shift, change shape, or both.</p> <p>While the benchmark is naturally a maximization problem, this implementation negates the fitness value upon evaluation, allowing it to be used directly with standard minimization algorithms.</p> <p>The Constrained Moving Peaks Benchmark (CMPB).</p> <p>This module provides an implementation of the CMPB generator, a dynamic and constrained optimization problem.</p>"},{"location":"lib/problem/#cilpy.problem.mpb--the-28-standard-problem-classes-the-generate_mpb_configs-function","title":"The 28 Standard Problem Classes &amp; The <code>generate_mpb_configs</code> Function","text":"<p>This module also includes the <code>generate_mpb_configs</code> helper function, which programmatically creates parameter dictionaries for the 28 standard problem classes defined by Duhain and Engelbrecht.</p> <p>These classes are identified by a 3-letter acronym (e.g., 'A1L'), which combines one code from each of the three classification schemes detailed below.</p>"},{"location":"lib/problem/#cilpy.problem.mpb--1-duhain-engelbrecht-spatial-and-temporal-severity-first-letter","title":"1. Duhain &amp; Engelbrecht: Spatial and Temporal Severity (First Letter)","text":"<p>Defines the magnitude and frequency of changes.</p> <ul> <li> <p>Progressive ('P'): Frequent, small changes.</p> <ul> <li><code>change_frequency</code>: Low value (high temporal change).</li> <li><code>change_severity</code> (<code>s</code>): Low value.</li> <li><code>height_severity</code>: Low value.</li> </ul> </li> <li> <p>Abrupt ('A'): Infrequent, large changes.</p> <ul> <li><code>change_frequency</code>: High value (low temporal change).</li> <li><code>change_severity</code> (<code>s</code>): High value.</li> <li><code>height_severity</code>: High value.</li> </ul> </li> <li> <p>Chaotic ('C'): Frequent, large changes.</p> <ul> <li><code>change_frequency</code>: Low value (high temporal change).</li> <li><code>change_severity</code> (<code>s</code>): High value.</li> <li><code>height_severity</code>: High value.</li> </ul> </li> </ul>"},{"location":"lib/problem/#cilpy.problem.mpb--2-hu-eberhart-shi-eberhart-optima-modification-second-letter","title":"2. Hu &amp; Eberhart / Shi &amp; Eberhart: Optima Modification (Second Letter)","text":"<p>Defines what changes about the peaks (position, value, or both).</p> <ul> <li> <p>Type I ('1'): Locations change, heights are constant.</p> <ul> <li><code>height_severity</code>: Set to 0.0.</li> <li>Requires <code>change_severity</code> (<code>s</code>) != 0 for movement.</li> </ul> </li> <li> <p>Type II ('2'): Locations are static, heights change.</p> <ul> <li><code>height_severity</code>: Set to a non-zero value.</li> <li>Requires <code>change_severity</code> (<code>s</code>) = 0 to prevent movement.</li> </ul> </li> <li> <p>Type III ('3'): Both locations and heights change.</p> <ul> <li><code>height_severity</code>: Set to a non-zero value.</li> <li>Requires <code>change_severity</code> (<code>s</code>) != 0 for movement.</li> </ul> </li> </ul>"},{"location":"lib/problem/#cilpy.problem.mpb--3-angeline-optima-trajectory-third-letter","title":"3. Angeline: Optima Trajectory (Third Letter)","text":"<p>Defines the pattern of peak movement.</p> <ul> <li> <p>Linear ('L'): Peaks move in a straight, correlated line.</p> <ul> <li><code>lambda_param</code>: Set to 1.0.</li> <li>Requires <code>change_severity</code> (<code>s</code>) != 0 for movement.</li> </ul> </li> <li> <p>Circular ('C'): Peaks have a periodic movement pattern. This is achieved in     the parameterization by preventing translational movement.</p> <ul> <li>Requires <code>change_severity</code> (<code>s</code>) = 0.</li> </ul> </li> <li> <p>Random ('R'): Peaks move randomly without a discernible pattern.</p> <ul> <li><code>lambda_param</code>: Set to 0.0.</li> <li>Requires <code>change_severity</code> (<code>s</code>) != 0 for movement.</li> </ul> </li> </ul>"},{"location":"lib/problem/#cilpy.problem.mpb--conflict-resolution","title":"Conflict Resolution","text":"<p>Some combinations are impossible (e.g., a Type II problem, which requires <code>s = 0</code>, cannot have Linear movement, which requires <code>s != 0</code>). The <code>generate_mpb_configs</code> function marks these impossible configurations by setting the <code>change_severity</code> parameter to the string 'XXX'.</p>"},{"location":"lib/problem/#cilpy.problem.mpb.MovingPeaksBenchmark","title":"<code>MovingPeaksBenchmark</code>","text":"<p>               Bases: <code>Problem[ndarray, float]</code></p> <p>An implementation of the Moving Peaks Benchmark (MPB) generator.</p> <p>This class conforms to the <code>Problem</code> interface and produces dynamic, unconstrained optimization problems. The objective is to find the maximum value in a landscape composed of several moving peaks.</p> Note <p>Since most solvers are minimizers, the <code>evaluate</code> method returns the negated value of the MPB function. Minimizing this value is equivalent to maximizing the original function.</p> <p>Attributes:</p> Name Type Description <code>peaks</code> <code>List[_Peak]</code> <p>A list of the peak objects in the landscape.</p> Source code in <code>cilpy/problem/mpb.py</code> <pre><code>class MovingPeaksBenchmark(Problem[np.ndarray, float]):\n    \"\"\"An implementation of the Moving Peaks Benchmark (MPB) generator.\n\n    This class conforms to the `Problem` interface and produces dynamic,\n    unconstrained optimization problems. The objective is to find the maximum\n    value in a landscape composed of several moving peaks.\n\n    Note:\n        Since most solvers are minimizers, the `evaluate` method returns the\n        *negated* value of the MPB function. Minimizing this value is\n        equivalent to maximizing the original function.\n\n    Attributes:\n        peaks (List[_Peak]): A list of the peak objects in the landscape.\n    \"\"\"\n\n    def __init__(\n        self,\n        dimension: int = 2,\n        num_peaks: int = 10,\n        domain: Tuple[float, float] = (0.0, 100.0),\n        min_height: float = 30.0,\n        max_height: float = 70.0,\n        min_width: float = 1.0,\n        max_width: float = 12.0,\n        change_frequency: int = 5000,\n        change_severity: float = 1.0,\n        height_severity: float = 7.0,\n        width_severity: float = 1.0,\n        lambda_param: float = 0.0,\n        name: str = \"MovingPeaksBenchmark\",\n    ):\n        \"\"\"Initializes the Moving Peaks Benchmark problem.\n\n        Args:\n            dimension (int): The dimensionality of the search landscape.\n            num_peaks (int): The number of peaks in the landscape.\n            domain (Tuple[float, float]): The `(min, max)` coordinates for the\n                symmetric search space.\n            min_height (float): The minimum initial height of a peak.\n            max_height (float): The maximum initial height of a peak.\n            min_width (float): The minimum initial width of a peak.\n            max_width (float): The maximum initial width of a peak.\n            change_frequency (int): The number of evaluations between landscape changes.\n            change_severity (float): Controls how severely peak positions change.\n            height_severity (float): Controls how severely peak heights change.\n            width_severity (float): Controls how severely peak widths change.\n            lambda_param (float): Correlates peak movement over time. A value of\n                0.0 results in random movement direction at each change.\n            name (str): The name of the problem instance.\n        \"\"\"\n        min_bounds = np.array([domain[0]] * dimension)\n        max_bounds = np.array([domain[1]] * dimension)\n        super().__init__(dimension, (min_bounds, max_bounds), name)\n\n        self._change_frequency = change_frequency\n        self._change_sev = change_severity\n        self._height_sev = height_severity\n        self._width_sev = width_severity\n        self._lambda = lambda_param\n        self._max_height = max_height\n\n        self.peaks: List[_Peak] = []\n        for _ in range(num_peaks):\n            pos = np.random.uniform(domain[0], domain[1], size=dimension)\n            height = random.uniform(min_height, max_height)\n            width = random.uniform(min_width, max_width)\n            self.peaks.append(_Peak(pos, height, width))\n\n        self._base_value = 0.0  # As per Equation 4.2\n        self._eval_count = 0\n        self._iteration_count = 0\n\n    def evaluate(self, solution: np.ndarray) -&gt; Evaluation[float]:\n        \"\"\"Evaluates a solution and returns its fitness.\n\n        This method checks if the environment should change based on the\n        evaluation count. It then calculates the function value as the maximum\n        of all peak evaluations.\n\n        Args:\n            solution (np.ndarray): The candidate solution to be evaluated.\n\n        Returns:\n            Evaluation[float]: An Evaluation object containing the negated\n                fitness value for use with minimization solvers.\n        \"\"\"\n        self._eval_count += 1\n\n        peak_values = [p.evaluate(solution) for p in self.peaks]\n        fitness = float(max([self._base_value] + peak_values))\n        return Evaluation(fitness=-fitness)\n\n    def is_dynamic(self) -&gt; Tuple[bool, bool]:\n        \"\"\"Indicates that the problem's objectives are dynamic.\n\n        Returns:\n            Tuple[bool, bool]: A tuple `(True, False)` as the objective\n                function changes over time but there are no constraints.\n        \"\"\"\n        return (True, False)\n\n    def is_multi_objective(self) -&gt; bool:\n        \"\"\"Indicates that the problem is not multi-objective.\"\"\"\n        return False\n\n    def begin_iteration(self) -&gt; None:\n        \"\"\"\n        This method is called by the runner once per iteration.\n        It handles the logic for changing the environment.\n        \"\"\"\n        self._iteration_count += 1\n\n        if (\n            self._change_frequency &gt; 0\n            and self._iteration_count &gt; 0\n            and self._iteration_count % self._change_frequency == 0\n        ):\n            self.update_all_peaks()\n\n    def get_fitness_bounds(self) -&gt; Tuple[float, float]:\n        \"\"\"\n        Returns the known theoretical min and max fitness values for the\n        problem.\n\n        This is used for calculating normalized performance metrics.\n\n        Returns:\n            A tuple containing (global_minimum_fitness, global_maximum_fitness).\n        \"\"\"\n        return (-self._max_height, -self._base_value)\n\n    def update_all_peaks(self) -&gt; None:\n        \"\"\"Updates all peaks of the mpb.\"\"\"\n        for peak in self.peaks:\n            peak.update(\n                height_sev=self._height_sev,\n                width_sev=self._width_sev,\n                change_sev=self._change_sev,\n                lambda_param=self._lambda,\n                bounds=self.bounds,\n                max_height_cap=self._max_height,\n            )\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.mpb.MovingPeaksBenchmark.__init__","title":"<code>__init__(dimension=2, num_peaks=10, domain=(0.0, 100.0), min_height=30.0, max_height=70.0, min_width=1.0, max_width=12.0, change_frequency=5000, change_severity=1.0, height_severity=7.0, width_severity=1.0, lambda_param=0.0, name='MovingPeaksBenchmark')</code>","text":"<p>Initializes the Moving Peaks Benchmark problem.</p> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>int</code> <p>The dimensionality of the search landscape.</p> <code>2</code> <code>num_peaks</code> <code>int</code> <p>The number of peaks in the landscape.</p> <code>10</code> <code>domain</code> <code>Tuple[float, float]</code> <p>The <code>(min, max)</code> coordinates for the symmetric search space.</p> <code>(0.0, 100.0)</code> <code>min_height</code> <code>float</code> <p>The minimum initial height of a peak.</p> <code>30.0</code> <code>max_height</code> <code>float</code> <p>The maximum initial height of a peak.</p> <code>70.0</code> <code>min_width</code> <code>float</code> <p>The minimum initial width of a peak.</p> <code>1.0</code> <code>max_width</code> <code>float</code> <p>The maximum initial width of a peak.</p> <code>12.0</code> <code>change_frequency</code> <code>int</code> <p>The number of evaluations between landscape changes.</p> <code>5000</code> <code>change_severity</code> <code>float</code> <p>Controls how severely peak positions change.</p> <code>1.0</code> <code>height_severity</code> <code>float</code> <p>Controls how severely peak heights change.</p> <code>7.0</code> <code>width_severity</code> <code>float</code> <p>Controls how severely peak widths change.</p> <code>1.0</code> <code>lambda_param</code> <code>float</code> <p>Correlates peak movement over time. A value of 0.0 results in random movement direction at each change.</p> <code>0.0</code> <code>name</code> <code>str</code> <p>The name of the problem instance.</p> <code>'MovingPeaksBenchmark'</code> Source code in <code>cilpy/problem/mpb.py</code> <pre><code>def __init__(\n    self,\n    dimension: int = 2,\n    num_peaks: int = 10,\n    domain: Tuple[float, float] = (0.0, 100.0),\n    min_height: float = 30.0,\n    max_height: float = 70.0,\n    min_width: float = 1.0,\n    max_width: float = 12.0,\n    change_frequency: int = 5000,\n    change_severity: float = 1.0,\n    height_severity: float = 7.0,\n    width_severity: float = 1.0,\n    lambda_param: float = 0.0,\n    name: str = \"MovingPeaksBenchmark\",\n):\n    \"\"\"Initializes the Moving Peaks Benchmark problem.\n\n    Args:\n        dimension (int): The dimensionality of the search landscape.\n        num_peaks (int): The number of peaks in the landscape.\n        domain (Tuple[float, float]): The `(min, max)` coordinates for the\n            symmetric search space.\n        min_height (float): The minimum initial height of a peak.\n        max_height (float): The maximum initial height of a peak.\n        min_width (float): The minimum initial width of a peak.\n        max_width (float): The maximum initial width of a peak.\n        change_frequency (int): The number of evaluations between landscape changes.\n        change_severity (float): Controls how severely peak positions change.\n        height_severity (float): Controls how severely peak heights change.\n        width_severity (float): Controls how severely peak widths change.\n        lambda_param (float): Correlates peak movement over time. A value of\n            0.0 results in random movement direction at each change.\n        name (str): The name of the problem instance.\n    \"\"\"\n    min_bounds = np.array([domain[0]] * dimension)\n    max_bounds = np.array([domain[1]] * dimension)\n    super().__init__(dimension, (min_bounds, max_bounds), name)\n\n    self._change_frequency = change_frequency\n    self._change_sev = change_severity\n    self._height_sev = height_severity\n    self._width_sev = width_severity\n    self._lambda = lambda_param\n    self._max_height = max_height\n\n    self.peaks: List[_Peak] = []\n    for _ in range(num_peaks):\n        pos = np.random.uniform(domain[0], domain[1], size=dimension)\n        height = random.uniform(min_height, max_height)\n        width = random.uniform(min_width, max_width)\n        self.peaks.append(_Peak(pos, height, width))\n\n    self._base_value = 0.0  # As per Equation 4.2\n    self._eval_count = 0\n    self._iteration_count = 0\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.mpb.MovingPeaksBenchmark.begin_iteration","title":"<code>begin_iteration()</code>","text":"<p>This method is called by the runner once per iteration. It handles the logic for changing the environment.</p> Source code in <code>cilpy/problem/mpb.py</code> <pre><code>def begin_iteration(self) -&gt; None:\n    \"\"\"\n    This method is called by the runner once per iteration.\n    It handles the logic for changing the environment.\n    \"\"\"\n    self._iteration_count += 1\n\n    if (\n        self._change_frequency &gt; 0\n        and self._iteration_count &gt; 0\n        and self._iteration_count % self._change_frequency == 0\n    ):\n        self.update_all_peaks()\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.mpb.MovingPeaksBenchmark.evaluate","title":"<code>evaluate(solution)</code>","text":"<p>Evaluates a solution and returns its fitness.</p> <p>This method checks if the environment should change based on the evaluation count. It then calculates the function value as the maximum of all peak evaluations.</p> <p>Parameters:</p> Name Type Description Default <code>solution</code> <code>ndarray</code> <p>The candidate solution to be evaluated.</p> required <p>Returns:</p> Type Description <code>Evaluation[float]</code> <p>Evaluation[float]: An Evaluation object containing the negated fitness value for use with minimization solvers.</p> Source code in <code>cilpy/problem/mpb.py</code> <pre><code>def evaluate(self, solution: np.ndarray) -&gt; Evaluation[float]:\n    \"\"\"Evaluates a solution and returns its fitness.\n\n    This method checks if the environment should change based on the\n    evaluation count. It then calculates the function value as the maximum\n    of all peak evaluations.\n\n    Args:\n        solution (np.ndarray): The candidate solution to be evaluated.\n\n    Returns:\n        Evaluation[float]: An Evaluation object containing the negated\n            fitness value for use with minimization solvers.\n    \"\"\"\n    self._eval_count += 1\n\n    peak_values = [p.evaluate(solution) for p in self.peaks]\n    fitness = float(max([self._base_value] + peak_values))\n    return Evaluation(fitness=-fitness)\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.mpb.MovingPeaksBenchmark.get_fitness_bounds","title":"<code>get_fitness_bounds()</code>","text":"<p>Returns the known theoretical min and max fitness values for the problem.</p> <p>This is used for calculating normalized performance metrics.</p> <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>A tuple containing (global_minimum_fitness, global_maximum_fitness).</p> Source code in <code>cilpy/problem/mpb.py</code> <pre><code>def get_fitness_bounds(self) -&gt; Tuple[float, float]:\n    \"\"\"\n    Returns the known theoretical min and max fitness values for the\n    problem.\n\n    This is used for calculating normalized performance metrics.\n\n    Returns:\n        A tuple containing (global_minimum_fitness, global_maximum_fitness).\n    \"\"\"\n    return (-self._max_height, -self._base_value)\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.mpb.MovingPeaksBenchmark.is_dynamic","title":"<code>is_dynamic()</code>","text":"<p>Indicates that the problem's objectives are dynamic.</p> <p>Returns:</p> Type Description <code>Tuple[bool, bool]</code> <p>Tuple[bool, bool]: A tuple <code>(True, False)</code> as the objective function changes over time but there are no constraints.</p> Source code in <code>cilpy/problem/mpb.py</code> <pre><code>def is_dynamic(self) -&gt; Tuple[bool, bool]:\n    \"\"\"Indicates that the problem's objectives are dynamic.\n\n    Returns:\n        Tuple[bool, bool]: A tuple `(True, False)` as the objective\n            function changes over time but there are no constraints.\n    \"\"\"\n    return (True, False)\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.mpb.MovingPeaksBenchmark.is_multi_objective","title":"<code>is_multi_objective()</code>","text":"<p>Indicates that the problem is not multi-objective.</p> Source code in <code>cilpy/problem/mpb.py</code> <pre><code>def is_multi_objective(self) -&gt; bool:\n    \"\"\"Indicates that the problem is not multi-objective.\"\"\"\n    return False\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.mpb.MovingPeaksBenchmark.update_all_peaks","title":"<code>update_all_peaks()</code>","text":"<p>Updates all peaks of the mpb.</p> Source code in <code>cilpy/problem/mpb.py</code> <pre><code>def update_all_peaks(self) -&gt; None:\n    \"\"\"Updates all peaks of the mpb.\"\"\"\n    for peak in self.peaks:\n        peak.update(\n            height_sev=self._height_sev,\n            width_sev=self._width_sev,\n            change_sev=self._change_sev,\n            lambda_param=self._lambda,\n            bounds=self.bounds,\n            max_height_cap=self._max_height,\n        )\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.mpb.demonstrate_mpb","title":"<code>demonstrate_mpb(params)</code>","text":"<p>Helper function to run and print a scenario.</p> Source code in <code>cilpy/problem/mpb.py</code> <pre><code>def demonstrate_mpb(params: dict):\n    \"\"\"Helper function to run and print a scenario.\"\"\"\n    print(\"-\" * 50)\n    print(f\"Demonstration: {params.get('name')}\")\n    print(\"-\" * 50)\n\n    # Instantiate the problem\n    problem = MovingPeaksBenchmark(**params)\n\n    # We will track the position and value of a single peak to see how it moves.\n    tracked_peak_index = 0\n\n    # We will also evaluate a static point to see how the landscape changes underneath it.\n    static_point_to_test = np.array([50.0] * params.get(\"dimension\"))  # type: ignore\n\n    num_changes_to_observe = 5\n    total_evaluations = params[\"change_frequency\"] * num_changes_to_observe\n\n    for i in range(total_evaluations + 1):\n        # The actual evaluation triggers the internal counter\n        evaluation = problem.evaluate(static_point_to_test)\n        problem.begin_iteration()\n\n        # Check if the environment just changed\n        if i &gt; 0 and i % (params[\"change_frequency\"] / 2) == 0:\n            change_num = i // params[\"change_frequency\"]\n            peak_pos = problem.peaks[tracked_peak_index].v\n            peak_evaluation = problem.evaluate(peak_pos)\n\n            print(f\"\\nEnvironment Change #{change_num} (at evaluation {i}):\")\n            print(f\"  - Position of Peak {tracked_peak_index}: {peak_pos}\")\n            print(\n                f\"  - Value of Peak {tracked_peak_index}: [{peak_evaluation.fitness}]\"\n            )\n            print(f\"  - Value at static point: {evaluation.fitness:.2f}\")\n\n    print(\"\\n\")\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.mpb.generate_mpb_configs","title":"<code>generate_mpb_configs(dimension=5, num_peaks=10, domain=(0.0, 100.0), min_height=30.0, max_height=70.0, min_width=1.0, max_width=12.0, width_severity=0.05, s_for_random=1.0)</code>","text":"<p>Programmatically generates parameter dictionaries for all 28 MPB classes.</p> <p>This function combines the rules from three classification schemes to generate 27 dynamic problem configurations and 1 static configuration. It handles contradictions between rules as specified.</p> <p>Parameters:</p> Name Type Description Default <code>s_for_random</code> <code>float</code> <p>The non-zero value to use for the change_severity parameter <code>s</code> when a non-zero value is required. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dict[str, Dict]: A dictionary where keys are the 3-letter acronyms (e.g., \"A1C\", \"P3L\") and values are the corresponding parameter dictionaries for the MovingPeaksBenchmark constructor. A \"STA\" key is included for the static case.</p> Source code in <code>cilpy/problem/mpb.py</code> <pre><code>def generate_mpb_configs(\n    dimension: int = 5,\n    num_peaks: int = 10,\n    domain: Tuple[float, float] = (0.0, 100.0),\n    min_height: float = 30.0,\n    max_height: float = 70.0,\n    min_width: float = 1.0,\n    max_width: float = 12.0,\n    width_severity: float = 0.05,\n    s_for_random: float = 1.0,  # s value for s != 0\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Programmatically generates parameter dictionaries for all 28 MPB classes.\n\n    This function combines the rules from three classification schemes to generate\n    27 dynamic problem configurations and 1 static configuration. It handles\n    contradictions between rules as specified.\n\n    Args:\n        s_for_random (float): The non-zero value to use for the change_severity\n            parameter `s` when a non-zero value is required. Defaults to 1.0.\n\n    Returns:\n        Dict[str, Dict]: A dictionary where keys are the 3-letter acronyms\n            (e.g., \"A1C\", \"P3L\") and values are the corresponding parameter\n            dictionaries for the MovingPeaksBenchmark constructor. A \"STA\" key\n            is included for the static case.\n    \"\"\"\n    if s_for_random == 0:\n        raise ValueError(\"'s_for_random' must be a non-zero value.\")\n\n    # 1. Base Configuration (common to all classes)\n    base_params = {\n        \"dimension\": dimension,\n        \"num_peaks\": num_peaks,\n        \"domain\": domain,\n        \"min_height\": min_height,\n        \"max_height\": max_height,\n        \"min_width\": min_width,\n        \"max_width\": max_width,\n        \"width_severity\": width_severity,  # Often kept low\n    }\n\n    # 2. Define \"Low\" vs. \"High\" Values for severity and frequency\n    LOW_S, HIGH_S = s_for_random, 10.0\n    LOW_H, HIGH_H = 1.0, 10.0\n\n    # High temporal frequency = low number of evaluations between changes\n    FREQ_PROGRESSIVE = 20\n    FREQ_ABRUPT = 100\n    FREQ_CHAOTIC = 30\n\n    # 3. Classification Rules\n    # Duhain &amp; Engelbrecht: Severity (Spatial &amp; Temporal)\n    # Acronyms: P (Progressive), A (Abrupt), C (Chaotic)\n    SEVERITY_CLASSES = {\n        \"P\": {\n            \"change_severity\": LOW_S,\n            \"height_severity\": LOW_H,\n            \"change_frequency\": FREQ_PROGRESSIVE,\n        },\n        \"A\": {\n            \"change_severity\": HIGH_S,\n            \"height_severity\": HIGH_H,\n            \"change_frequency\": FREQ_ABRUPT,\n        },\n        \"C\": {\n            \"change_severity\": HIGH_S,\n            \"height_severity\": HIGH_H,\n            \"change_frequency\": FREQ_CHAOTIC,\n        },\n    }\n\n    # Hu &amp; Eberhart / Shi &amp; Eberhart: Optima Modification\n    # Acronyms: 1 (Type I), 2 (Type II), 3 (Type III)\n    # We use 's_req' to define the requirement for the change_severity (s)\n    MODIFICATION_CLASSES = {\n        \"1\": {\"height_severity\": 0.0, \"s_req\": \"!=0\"},\n        \"2\": {\"s_req\": \"=0\"},  # height_severity will be taken from SEVERITY_CLASSES\n        \"3\": {\"s_req\": \"!=0\"},  # height_severity will be taken from SEVERITY_CLASSES\n    }\n\n    # Angeline: Optima Trajectory\n    # Acronyms: L (Linear), C (Circular), R (Random)\n    MOVEMENT_CLASSES = {\n        \"L\": {\"lambda_param\": 1.0, \"s_req\": \"!=0\"},\n        \"C\": {\"lambda_param\": 0.0, \"s_req\": \"=0\"},  # lambda is irrelevant when s=0\n        \"R\": {\"lambda_param\": 0.0, \"s_req\": \"!=0\"},\n    }\n\n    # --- Generation Logic ---\n    all_configs = {}\n\n    # 4. Iterate through all 3*3*3 = 27 combinations\n    severity_codes = SEVERITY_CLASSES.keys()\n    modification_codes = MODIFICATION_CLASSES.keys()\n    movement_codes = MOVEMENT_CLASSES.keys()\n\n    for sev_code, mod_code, mov_code in itertools.product(\n        severity_codes, modification_codes, movement_codes\n    ):\n        acronym = f\"{sev_code}{mod_code}{mov_code}\"\n\n        # Start with base and add severity parameters\n        config = base_params.copy()\n        config.update(SEVERITY_CLASSES[sev_code])\n        config[\"name\"] = acronym\n\n        mod_rules = MODIFICATION_CLASSES[mod_code]\n        mov_rules = MOVEMENT_CLASSES[mov_code]\n\n        # 5. Resolve Conflicts for `change_severity` (s)\n        s_req_mod = mod_rules[\"s_req\"]\n        s_req_mov = mov_rules[\"s_req\"]\n\n        is_conflict = (s_req_mod == \"!=0\" and s_req_mov == \"=0\") or (\n            s_req_mod == \"=0\" and s_req_mov == \"!=0\"\n        )\n\n        if is_conflict:\n            # *2L/*2R: 2 requires s = 0, but L&amp;R requires s != 0\n            #          2 gets priority since *3* requires s != 0\n            if mod_code == \"2\" and (mov_code == \"L\" or mov_code == \"R\"):\n                config[\"change_severity\"] = 0.0\n            # *1C/*3C: C requires s = 0, but 1&amp;3 requires s != 0\n            #          C gets priority since *2* requires s = 0\n            elif mov_code == \"C\" and (mod_code == \"1\" or mod_code == \"3\"):\n                config[\"change_severity\"] = 1.0\n            else:\n                # This is not supposed to happen, but is kept for clarity\n                config[\"change_severity\"] = \"XXX\"  # Assign placeholder on conflict\n        elif s_req_mod == \"=0\" or s_req_mov == \"=0\":\n            # If either requires s=0 and there's no conflict, it must be 0\n            config[\"change_severity\"] = 0.0\n        else:\n            # Otherwise, s must be non-zero. Use the value from the severity class.\n            # This is already set, but we make it explicit for clarity.\n            pass\n\n        # 6. Apply overrides from modification and movement rules\n        # C1*: C requires hSeverity high, but 1 requires hSeverity = 0\n        #      1 (mod rule) gets priority since *2*/*3* requires hSeverity != 0\n        if \"height_severity\" in mod_rules:\n            config[\"height_severity\"] = mod_rules[\"height_severity\"]\n\n        if \"lambda_param\" in mov_rules:\n            config[\"lambda_param\"] = mov_rules[\"lambda_param\"]\n\n        all_configs[acronym] = config\n\n    # Add the static problem class\n    static_config = base_params.copy()\n    static_config.update(\n        {\n            \"change_frequency\": 0,\n            \"change_severity\": 0,\n            \"height_severity\": 0,\n            \"width_severity\": 0,\n            \"lambda_param\": 0,\n            \"name\": \"STA\",\n        }\n    )\n    all_configs[\"STA\"] = static_config\n\n    return all_configs\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.cmpb.ConstrainedMovingPeaksBenchmark","title":"<code>ConstrainedMovingPeaksBenchmark</code>","text":"<p>               Bases: <code>Problem[ndarray, float]</code></p> <p>An implementation of the Constrained Moving Peaks Benchmark (CMPB).</p> <p>This class generates a dynamic constrained optimization problem by composing two independent <code>MovingPeaksBenchmark</code> instances: one for the objective function landscape (<code>f</code>) and one for the constraint landscape (<code>g</code>).</p> <p>The problem is naturally a maximization problem defined as: Maximize: h(x) = f(x) - g(x) A solution is considered feasible if h(x) &gt;= 0 (i.e., f(x) &gt;= g(x)).</p> <p>To align with standard minimization solvers, this class formulates the problem as: Minimize: g(x) - f(x) Subject to: g(x) - f(x) &lt;= 0</p> <p>This formulation correctly models the problem, where the objective function and the single inequality constraint are the same.</p> <p>Attributes:</p> Name Type Description <code>f_landscape</code> <code>MovingPeaksBenchmark</code> <p>The MPB instance for the objective function landscape.</p> <code>g_landscape</code> <code>MovingPeaksBenchmark</code> <p>The MPB instance for the constraint function landscape.</p> Source code in <code>cilpy/problem/cmpb.py</code> <pre><code>class ConstrainedMovingPeaksBenchmark(Problem[np.ndarray, float]):\n    \"\"\"An implementation of the Constrained Moving Peaks Benchmark (CMPB).\n\n    This class generates a dynamic constrained optimization problem by composing\n    two independent `MovingPeaksBenchmark` instances: one for the objective\n    function landscape (`f`) and one for the constraint landscape (`g`).\n\n    The problem is naturally a maximization problem defined as:\n    Maximize: h(x) = f(x) - g(x)\n    A solution is considered feasible if h(x) &gt;= 0 (i.e., f(x) &gt;= g(x)).\n\n    To align with standard minimization solvers, this class formulates the\n    problem as:\n    Minimize: g(x) - f(x)\n    Subject to: g(x) - f(x) &lt;= 0\n\n    This formulation correctly models the problem, where the objective function\n    and the single inequality constraint are the same.\n\n    Attributes:\n        f_landscape (MovingPeaksBenchmark): The MPB instance for the objective\n            function landscape.\n        g_landscape (MovingPeaksBenchmark): The MPB instance for the constraint\n            function landscape.\n    \"\"\"\n\n    def __init__(\n        self,\n        f_params: Dict[str, Any],\n        g_params: Dict[str, Any],\n        name: str = \"ConstrainedMovingPeaksBenchmark\",\n    ):\n        \"\"\"Initializes the Constrained Moving Peaks Benchmark generator.\n\n        Args:\n            f_params (Dict[str, Any]): A dictionary of parameters for the\n                objective landscape (f), which will be passed to the\n                `MovingPeaksBenchmark` constructor.\n            g_params (Dict[str, Any]): A dictionary of parameters for the\n                constraint landscape (g), which will be passed to the\n                `MovingPeaksBenchmark` constructor.\n            name (str): The name for the problem instance.\n\n        Raises:\n            ValueError: If the 'dimension' parameter is not specified or is\n                different for `f_params` and `g_params`.\n        \"\"\"\n        f_dim = f_params.get(\"dimension\")\n        g_dim = g_params.get(\"dimension\")\n\n        if f_dim is None or g_dim is None or f_dim != g_dim:\n            raise ValueError(\n                \"The 'dimension' parameter must be specified and identical for \"\n                \"both f_params and g_params.\"\n            )\n\n        self.f_landscape = MovingPeaksBenchmark(**f_params)\n        self.g_landscape = MovingPeaksBenchmark(**g_params)\n\n        # The problem's domain is defined by the 'f' landscape.\n        super().__init__(\n            dimension=self.f_landscape.dimension,\n            bounds=self.f_landscape.bounds,\n            name=name,\n        )\n\n        # Determine if landscapes are dynamic based on their change frequency.\n        self._is_f_dynamic = f_params.get(\"change_frequency\", 0) &gt; 0\n        self._is_g_dynamic = g_params.get(\"change_frequency\", 0) &gt; 0\n\n    def evaluate(self, solution: np.ndarray) -&gt; Evaluation[float]:\n        \"\"\"Evaluates a solution against the composed objective and constraint.\n\n        This method calls the `evaluate` method of the underlying `f` and `g`\n        landscapes exactly once, ensuring that their internal evaluation\n        counters are updated correctly. It then composes the results to form the\n        final fitness and constraint violation.\n\n        Args:\n            solution (np.ndarray): The candidate solution to be evaluated.\n\n        Returns:\n            Evaluation[float]: An Evaluation object containing the composed\n                fitness and the single inequality constraint violation.\n        \"\"\"\n        # Evaluate each landscape once. This triggers their internal update\n        # logic and returns the negated maximization value.\n        f_eval = self.f_landscape.evaluate(solution)\n        g_eval = self.g_landscape.evaluate(solution)\n\n        # The MPB implementation in cilpy is already a minimization solver,\n        # hence we convert evaluations back to the original maximization values.\n        f_val = -f_eval.fitness\n        g_val = -g_eval.fitness\n\n        # The objective for maximization is h(x) = f(x) - g(x).  Infeasible\n        # areas are indicated where h(x) &lt; 0.\n        composed_fitness = f_val - g_val\n\n        # Fitness is negated for minimization solvers\n        return Evaluation(\n            fitness=-composed_fitness, constraints_inequality=[-composed_fitness]\n        )\n\n    def is_dynamic(self) -&gt; Tuple[bool, bool]:\n        \"\"\"Indicates whether the objective or constraint landscapes can change.\n\n        The composed objective `g(x) - f(x)` is dynamic if either `f` or `g`\n        is dynamic. Similarly, the composed constraint is dynamic if either\n        `f` or `g` is dynamic.\n\n        Returns:\n            Tuple[bool, bool]: A tuple `(is_objective_dynamic, is_constraint_dynamic)`.\n        \"\"\"\n        return (self._is_f_dynamic, self._is_g_dynamic)\n\n    def is_multi_objective(self) -&gt; bool:\n        return False\n\n    def begin_iteration(self) -&gt; None:\n        \"\"\"\n        Notifies the underlying landscapes that a new solver iteration is\n        beginning.\n\n        This method acts as a delegate, calling the `begin_iteration` method on\n        both the objective (f) and constraint (g) landscapes. This ensures\n        that their internal iteration counters are incremented and environmental\n        changes are triggered correctly and in sync.\n        \"\"\"\n        self.f_landscape.begin_iteration()\n        self.g_landscape.begin_iteration()\n\n    def get_fitness_bounds(self) -&gt; Tuple[float, float]:\n        \"\"\"\n        Returns the known theoretical min and max fitness values for the\n        problem.\n\n        This is used for calculating normalized performance metrics.\n\n        Returns:\n            A tuple containing (global_minimum_fitness, global_maximum_fitness).\n        \"\"\"\n        return (-self.f_landscape._max_height, -0.0)\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.cmpb.ConstrainedMovingPeaksBenchmark.__init__","title":"<code>__init__(f_params, g_params, name='ConstrainedMovingPeaksBenchmark')</code>","text":"<p>Initializes the Constrained Moving Peaks Benchmark generator.</p> <p>Parameters:</p> Name Type Description Default <code>f_params</code> <code>Dict[str, Any]</code> <p>A dictionary of parameters for the objective landscape (f), which will be passed to the <code>MovingPeaksBenchmark</code> constructor.</p> required <code>g_params</code> <code>Dict[str, Any]</code> <p>A dictionary of parameters for the constraint landscape (g), which will be passed to the <code>MovingPeaksBenchmark</code> constructor.</p> required <code>name</code> <code>str</code> <p>The name for the problem instance.</p> <code>'ConstrainedMovingPeaksBenchmark'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the 'dimension' parameter is not specified or is different for <code>f_params</code> and <code>g_params</code>.</p> Source code in <code>cilpy/problem/cmpb.py</code> <pre><code>def __init__(\n    self,\n    f_params: Dict[str, Any],\n    g_params: Dict[str, Any],\n    name: str = \"ConstrainedMovingPeaksBenchmark\",\n):\n    \"\"\"Initializes the Constrained Moving Peaks Benchmark generator.\n\n    Args:\n        f_params (Dict[str, Any]): A dictionary of parameters for the\n            objective landscape (f), which will be passed to the\n            `MovingPeaksBenchmark` constructor.\n        g_params (Dict[str, Any]): A dictionary of parameters for the\n            constraint landscape (g), which will be passed to the\n            `MovingPeaksBenchmark` constructor.\n        name (str): The name for the problem instance.\n\n    Raises:\n        ValueError: If the 'dimension' parameter is not specified or is\n            different for `f_params` and `g_params`.\n    \"\"\"\n    f_dim = f_params.get(\"dimension\")\n    g_dim = g_params.get(\"dimension\")\n\n    if f_dim is None or g_dim is None or f_dim != g_dim:\n        raise ValueError(\n            \"The 'dimension' parameter must be specified and identical for \"\n            \"both f_params and g_params.\"\n        )\n\n    self.f_landscape = MovingPeaksBenchmark(**f_params)\n    self.g_landscape = MovingPeaksBenchmark(**g_params)\n\n    # The problem's domain is defined by the 'f' landscape.\n    super().__init__(\n        dimension=self.f_landscape.dimension,\n        bounds=self.f_landscape.bounds,\n        name=name,\n    )\n\n    # Determine if landscapes are dynamic based on their change frequency.\n    self._is_f_dynamic = f_params.get(\"change_frequency\", 0) &gt; 0\n    self._is_g_dynamic = g_params.get(\"change_frequency\", 0) &gt; 0\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.cmpb.ConstrainedMovingPeaksBenchmark.begin_iteration","title":"<code>begin_iteration()</code>","text":"<p>Notifies the underlying landscapes that a new solver iteration is beginning.</p> <p>This method acts as a delegate, calling the <code>begin_iteration</code> method on both the objective (f) and constraint (g) landscapes. This ensures that their internal iteration counters are incremented and environmental changes are triggered correctly and in sync.</p> Source code in <code>cilpy/problem/cmpb.py</code> <pre><code>def begin_iteration(self) -&gt; None:\n    \"\"\"\n    Notifies the underlying landscapes that a new solver iteration is\n    beginning.\n\n    This method acts as a delegate, calling the `begin_iteration` method on\n    both the objective (f) and constraint (g) landscapes. This ensures\n    that their internal iteration counters are incremented and environmental\n    changes are triggered correctly and in sync.\n    \"\"\"\n    self.f_landscape.begin_iteration()\n    self.g_landscape.begin_iteration()\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.cmpb.ConstrainedMovingPeaksBenchmark.evaluate","title":"<code>evaluate(solution)</code>","text":"<p>Evaluates a solution against the composed objective and constraint.</p> <p>This method calls the <code>evaluate</code> method of the underlying <code>f</code> and <code>g</code> landscapes exactly once, ensuring that their internal evaluation counters are updated correctly. It then composes the results to form the final fitness and constraint violation.</p> <p>Parameters:</p> Name Type Description Default <code>solution</code> <code>ndarray</code> <p>The candidate solution to be evaluated.</p> required <p>Returns:</p> Type Description <code>Evaluation[float]</code> <p>Evaluation[float]: An Evaluation object containing the composed fitness and the single inequality constraint violation.</p> Source code in <code>cilpy/problem/cmpb.py</code> <pre><code>def evaluate(self, solution: np.ndarray) -&gt; Evaluation[float]:\n    \"\"\"Evaluates a solution against the composed objective and constraint.\n\n    This method calls the `evaluate` method of the underlying `f` and `g`\n    landscapes exactly once, ensuring that their internal evaluation\n    counters are updated correctly. It then composes the results to form the\n    final fitness and constraint violation.\n\n    Args:\n        solution (np.ndarray): The candidate solution to be evaluated.\n\n    Returns:\n        Evaluation[float]: An Evaluation object containing the composed\n            fitness and the single inequality constraint violation.\n    \"\"\"\n    # Evaluate each landscape once. This triggers their internal update\n    # logic and returns the negated maximization value.\n    f_eval = self.f_landscape.evaluate(solution)\n    g_eval = self.g_landscape.evaluate(solution)\n\n    # The MPB implementation in cilpy is already a minimization solver,\n    # hence we convert evaluations back to the original maximization values.\n    f_val = -f_eval.fitness\n    g_val = -g_eval.fitness\n\n    # The objective for maximization is h(x) = f(x) - g(x).  Infeasible\n    # areas are indicated where h(x) &lt; 0.\n    composed_fitness = f_val - g_val\n\n    # Fitness is negated for minimization solvers\n    return Evaluation(\n        fitness=-composed_fitness, constraints_inequality=[-composed_fitness]\n    )\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.cmpb.ConstrainedMovingPeaksBenchmark.get_fitness_bounds","title":"<code>get_fitness_bounds()</code>","text":"<p>Returns the known theoretical min and max fitness values for the problem.</p> <p>This is used for calculating normalized performance metrics.</p> <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>A tuple containing (global_minimum_fitness, global_maximum_fitness).</p> Source code in <code>cilpy/problem/cmpb.py</code> <pre><code>def get_fitness_bounds(self) -&gt; Tuple[float, float]:\n    \"\"\"\n    Returns the known theoretical min and max fitness values for the\n    problem.\n\n    This is used for calculating normalized performance metrics.\n\n    Returns:\n        A tuple containing (global_minimum_fitness, global_maximum_fitness).\n    \"\"\"\n    return (-self.f_landscape._max_height, -0.0)\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.cmpb.ConstrainedMovingPeaksBenchmark.is_dynamic","title":"<code>is_dynamic()</code>","text":"<p>Indicates whether the objective or constraint landscapes can change.</p> <p>The composed objective <code>g(x) - f(x)</code> is dynamic if either <code>f</code> or <code>g</code> is dynamic. Similarly, the composed constraint is dynamic if either <code>f</code> or <code>g</code> is dynamic.</p> <p>Returns:</p> Type Description <code>Tuple[bool, bool]</code> <p>Tuple[bool, bool]: A tuple <code>(is_objective_dynamic, is_constraint_dynamic)</code>.</p> Source code in <code>cilpy/problem/cmpb.py</code> <pre><code>def is_dynamic(self) -&gt; Tuple[bool, bool]:\n    \"\"\"Indicates whether the objective or constraint landscapes can change.\n\n    The composed objective `g(x) - f(x)` is dynamic if either `f` or `g`\n    is dynamic. Similarly, the composed constraint is dynamic if either\n    `f` or `g` is dynamic.\n\n    Returns:\n        Tuple[bool, bool]: A tuple `(is_objective_dynamic, is_constraint_dynamic)`.\n    \"\"\"\n    return (self._is_f_dynamic, self._is_g_dynamic)\n</code></pre>"},{"location":"lib/problem/#cilpy.problem.cmpb.demonstrate_cmpb","title":"<code>demonstrate_cmpb(name, f_params, g_params)</code>","text":"<p>Helper function to run and print a constrained scenario.</p> Source code in <code>cilpy/problem/cmpb.py</code> <pre><code>def demonstrate_cmpb(name: str, f_params: Dict[str, Any], g_params: Dict[str, Any]):\n    \"\"\"Helper function to run and print a constrained scenario.\"\"\"\n    print(\"=\" * 60)\n    print(f\"Demonstration: {name}\")\n    print(\"=\" * 60)\n\n    # Instantiate the constrained problem\n    problem = ConstrainedMovingPeaksBenchmark(f_params, g_params)\n    change_frequency = max(\n        f_params.get(\"change_frequency\", 0), g_params.get(\"change_frequency\", 0)\n    )\n\n    if change_frequency == 0:\n        print(\"Both landscapes are static. No changes will occur.\")\n        return\n\n    # Define a few points to track their feasibility and fitness over time\n    test_points = {\n        \"Center\": np.array([50.0, 50.0]),\n        \"Corner\": np.array([10.0, 10.0]),\n    }\n\n    num_changes_to_observe = 5\n    total_evaluations = change_frequency * num_changes_to_observe\n\n    for i in range(total_evaluations + 1):\n        # 1. Notify the problem that a new iteration is beginning.\n        problem.begin_iteration()\n\n        # 2. Evaluate points in the (potentially new) landscape.\n        evals = {name: problem.evaluate(pos) for name, pos in test_points.items()}\n\n        if i &gt; 0 and i % (change_frequency / 2) == 0:\n            print(\n                f\"\\n--- Environment Change #{i // change_frequency} (at iteration {i}) ---\"\n            )\n            for name, evaluation in evals.items():\n                violation = evaluation.constraints_inequality[0]  # type: ignore\n                is_feasible = violation &lt;= 0\n                print(\n                    f\"  - Point '{name}': Fitness = {evaluation.fitness:.2f}, \"\n                    f\"Violation = {violation:.2f}, Feasible = {is_feasible}\"\n                )\n    print(\"\\n\")\n</code></pre>"},{"location":"lib/solver/","title":"Included Solvers","text":""},{"location":"lib/solver/#cilpy.solver.pso.PSO","title":"<code>cilpy.solver.pso.PSO</code>","text":"<p>               Bases: <code>Solver[List[float], float]</code></p> <p>A canonical Particle Swarm Optimization (PSO) solver.</p> <p>The implementation uses a global best (gbest) topology (star topology), and uses inertial weight.</p>"},{"location":"lib/solver/#cilpy.solver.pso.PSO.__init__","title":"<code>__init__(problem, name, swarm_size, w, c1, c2, **kwargs)</code>","text":"<p>Initializes the Particle Swarm Optimization solver.</p> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>Problem[List[float], float]</code> <p>The optimization problem to solve.</p> required <code>swarm_size</code> <code>int</code> <p>The number of particles in the swarm.</p> required <code>w</code> <code>float</code> <p>The inertia weight, controlling the influence of the previous velocity.</p> required <code>c1</code> <code>float</code> <p>The cognitive coefficient, scaling the influence of the particle's personal best.</p> required <code>c2</code> <code>float</code> <p>The social coefficient, scaling the influence of the swarm's global best.</p> required <code>**kwargs</code> <p>Additional keyword arguments (not used in this canonical PSO).</p> <code>{}</code>"},{"location":"lib/solver/#cilpy.solver.pso.PSO._initialize_positions","title":"<code>_initialize_positions()</code>","text":"<p>Creates the initial particle positions.</p>"},{"location":"lib/solver/#cilpy.solver.pso.PSO.step","title":"<code>step()</code>","text":"<p>Performs one iteration of the PSO algorithm.</p>"},{"location":"lib/solver/#cilpy.solver.pso.PSO.get_result","title":"<code>get_result()</code>","text":"<p>Returns the global best solution found by the swarm.</p>"},{"location":"lib/solver/#cilpy.solver.ga.GA","title":"<code>cilpy.solver.ga.GA</code>","text":"<p>               Bases: <code>Solver[List[float], float]</code></p> <p>A canonical Genetic Algorithm (GA) for single-objective optimization.</p> <p>The algorithm uses: - Tournament selection to choose parents. - Single-point crossover for reproduction. - Gaussian mutation to introduce genetic diversity. - Elitism to preserve the best solution across generations.</p>"},{"location":"lib/solver/#cilpy.solver.ga.GA.__init__","title":"<code>__init__(problem, name, population_size, crossover_rate, mutation_rate, tournament_size=2, **kwargs)</code>","text":"<p>Initializes the Genetic Algorithm solver.</p> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>Problem[List[float], float]</code> <p>The optimization problem to solve.</p> required <code>name</code> <code>str</code> <p>the name of the solver</p> required <code>population_size</code> <code>int</code> <p>The number of individuals in the population.</p> required <code>crossover_rate</code> <code>float</code> <p>The probability of crossover (pc) occurring between two parents.</p> required <code>mutation_rate</code> <code>float</code> <p>The probability of mutation (pm) for each gene in an offspring.</p> required <code>tournament_size</code> <code>int</code> <p>The number of individuals to select for each tournament. Defaults to 2.</p> <code>2</code> <code>**kwargs</code> <p>Additional keyword arguments (not used in this canonical GA).</p> <code>{}</code>"},{"location":"lib/solver/#cilpy.solver.ga.GA.step","title":"<code>step()</code>","text":"<p>Performs one generation of the Genetic Algorithm.</p>"},{"location":"lib/solver/#cilpy.solver.ga.GA.get_result","title":"<code>get_result()</code>","text":"<p>Returns the best solution found in the current population.</p>"},{"location":"lib/solver/#cilpy.solver.de.DE","title":"<code>cilpy.solver.de.DE</code>","text":"<p>               Bases: <code>Solver[List[float], float]</code></p> <p>A canonical Differential Evolution (DE) solver for single-objective optimization.</p> <p>This is a <code>DE/rand/1/bin</code> implementation. It creates a trial vector for each member of the population and replaces the member if the trial vector has better or equal fitness.</p> <p>The algorithm uses: - <code>rand</code> strategy for selecting vectors for mutation. - <code>1</code> difference vector in the mutation step. - <code>bin</code> (binomial) crossover.</p>"},{"location":"lib/solver/#cilpy.solver.de.DE.__init__","title":"<code>__init__(problem, name, population_size, crossover_rate, f_weight, **kwargs)</code>","text":"<p>Initializes the Differential Evolution solver.</p> <p>Parameters:</p> Name Type Description Default <code>problem</code> <code>Problem[List[float], float]</code> <p>The optimization problem to solve.</p> required <code>name</code> <code>str</code> <p>the name of the solver</p> required <code>population_size</code> <code>int</code> <p>The number of individuals (ns) in the population.</p> required <code>crossover_rate</code> <code>float</code> <p>The crossover probability (CR) in the range [0, 1].</p> required <code>f_weight</code> <code>float</code> <p>The differential weight (F) for mutation, typically in the range [0, 2].</p> required <code>**kwargs</code> <p>Additional keyword arguments (not used in this canonical DE).</p> <code>{}</code>"},{"location":"lib/solver/#cilpy.solver.de.DE.step","title":"<code>step()</code>","text":"<p>Performs one generation of the Differential Evolution algorithm.</p>"},{"location":"lib/solver/#cilpy.solver.de.DE.get_result","title":"<code>get_result()</code>","text":"<p>Returns the best solution found in the current population.</p>"}]}